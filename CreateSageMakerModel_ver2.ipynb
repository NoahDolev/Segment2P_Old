{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, re\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker.tensorflow.model import TensorFlowModel\n",
    "import tarfile\n",
    "import keras\n",
    "import numpy as np\n",
    "import os\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "from mrcnn import utils\n",
    "from mrcnn import model as modellib\n",
    "from mrcnn.config import Config\n",
    "from keras.models import model_from_json\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tarfile.open('/home/ec2-user/SageMaker/keras_model/exported_model/model0100.tar.gz', mode='w:gz') as archive:\n",
    "#     archive.add(\"/home/ec2-user/SageMaker/keras_model/savedmodel_0100/\", recursive=True)\n",
    "\n",
    "try:\n",
    "    !mkdir /tmp/export\n",
    "    !mkdir /tmp/export/servo\n",
    "    !mkdir /tmp/export/servo/0\n",
    "except:\n",
    "    pass\n",
    "\n",
    "!cp /home/ec2-user/SageMaker/keras_model/savedmodel_0100/saved_model.pb /tmp/export/servo/0\n",
    "!cp -r /home/ec2-user/SageMaker/keras_model/savedmodel_0100/variables /tmp/export/servo/variables\n",
    "with tarfile.open('/tmp/model0100.tar.gz', mode='w:gz') as archive:\n",
    "     archive.add(\"/tmp/export\",arcname=\"/export\", recursive=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "inputs = sagemaker_session.upload_data(path='/tmp/model0100.tar.gz',\n",
    "                                       key_prefix='model')\n",
    "\n",
    "sagemaker_model = TensorFlowModel(model_data = 's3://' + sagemaker_session.default_bucket() + '/model/model0100.tar.gz',\n",
    "                                  role = role,\n",
    "                                  framework_version='1.13', py_version='py3',\n",
    "                                  entry_point = 'train.py', image = '763104351884.dkr.ecr.eu-west-1.amazonaws.com/tensorflow-inference:1.13-cpu' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = sagemaker_model.deploy(initial_instance_count=1,\n",
    "                                   instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = \"tensorflow-inference-2019-07-09-11-32-10-037\"\n",
    "predictor=sagemaker.tensorflow.model.TensorFlowPredictor(endpoint_name, sagemaker_session)\n",
    "\n",
    "# from tifffile import imsave,imread\n",
    "# img = imread(\"190128_LD2_cleaned.tif\")\n",
    "\n",
    "# predictor.predict(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/ec2-user/SageMaker/images/\"\n",
    "class cellConfig(Config):\n",
    "    NAME = \"cell\"\n",
    "\n",
    "    # Adjust to GPU memory\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "    NUM_CLASSES = 2  # Background + cell\n",
    "\n",
    "    STEPS_PER_EPOCH = 45\n",
    "    VALIDATION_STEPS = 5\n",
    "\n",
    "    # Don't exclude based on confidence. Since we have two classes\n",
    "    # then 0.5 is the minimum anyway as it picks between nucleus and BG\n",
    "    DETECTION_MIN_CONFIDENCE = 0.5\n",
    "    #DETECTION_MIN_CONFIDENCE = 0.5\n",
    "\n",
    "    # Backbone network architecture\n",
    "    # Supported values are: resnet50, resnet101\n",
    "    #BACKBONE = \"resnet50\"\n",
    "    BACKBONE = \"resnet101\"\n",
    "\n",
    "    # Input image resizing\n",
    "    # Random crops of size 512x512\n",
    "    IMAGE_RESIZE_MODE = \"crop\"\n",
    "    IMAGE_MIN_DIM = 512\n",
    "    IMAGE_MAX_DIM = 512\n",
    "    IMAGE_MIN_SCALE = 2.0\n",
    "\n",
    "    # Length of square anchor side in pixels\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)\n",
    "\n",
    "    # ROIs kept after non-maximum supression (training and inference)\n",
    "    POST_NMS_ROIS_TRAINING = 1000\n",
    "    POST_NMS_ROIS_INFERENCE = 2000\n",
    "\n",
    "    # Non-max suppression threshold to filter RPN proposals.\n",
    "    # You can increase this during training to generate more propsals.\n",
    "    #RPN_NMS_THRESHOLD = 0.9\n",
    "    RPN_NMS_THRESHOLD=0.99\n",
    "\n",
    "    # How many anchors per image to use for RPN training\n",
    "    #RPN_TRAIN_ANCHORS_PER_IMAGE = 64\n",
    "    RPN_TRAIN_ANCHORS_PER_IMAGE = 128\n",
    "\n",
    "    # Image mean (RGB)\n",
    "    #MEAN_PIXEL = np.array([43.53, 39.56, 48.22])\n",
    "    MEAN_PIXEL = np.array([126,126,126])\n",
    "    # If enabled, resizes instance masks to a smaller size to reduce\n",
    "    # memory load. Recommended when using high-resolution images.\n",
    "    USE_MINI_MASK = True\n",
    "    #MINI_MASK_SHAPE = (56, 56)  # (height, width) of the mini-mask\n",
    "    MINI_MASK_SHAPE = (100,100)\n",
    "\n",
    "    # Number of ROIs per image to feed to classifier/mask heads\n",
    "    # The Mask RCNN paper uses 512 but often the RPN doesn't generate\n",
    "    # enough positive proposals to fill this and keep a positive:negative\n",
    "    # ratio of 1:3. You can increase the number of proposals by adjusting\n",
    "    # the RPN NMS threshold.\n",
    "    #TRAIN_ROIS_PER_IMAGE = 128\n",
    "    TRAIN_ROIS_PER_IMAGE = 256\n",
    "\n",
    "    # Maximum number of ground truth instances to use in one image\n",
    "    #MAX_GT_INSTANCES = 200\n",
    "    MAX_GT_INSTANCES = 500\n",
    "    # Max number of final detections per image\n",
    "    #DETECTION_MAX_INSTANCES = 400\n",
    "    DETECTION_MAX_INSTANCES = 1000\n",
    "\n",
    "class ImageDataset(utils.Dataset):\n",
    "    def load_images(self, dataset_dir):\n",
    "        \"\"\"\n",
    "        Loads dataset images.\n",
    "        :param dataset_dir: string, path to dataset directory.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.add_class(\"cell\", 1, \"cell\")\n",
    "\n",
    "        image_ids = [fn for fn in os.listdir(dataset_dir)\n",
    "                     if any(fn.endswith(ext) for ext in ['tif', \"png\"])]\n",
    "\n",
    "        for image_id in image_ids:\n",
    "            self.add_image(\n",
    "                'cell',\n",
    "                image_id=os.path.splitext(image_id)[0],\n",
    "                path=os.path.join(dataset_dir, image_id)\n",
    "            )\n",
    "            \n",
    "chunksize = 1\n",
    "class CellInferenceConfig(cellConfig):\n",
    "        # Set batch size to 1 to run one image at a time\n",
    "        GPU_COUNT = 1\n",
    "        IMAGES_PER_GPU = chunksize\n",
    "        # Don't resize imager for inferencing\n",
    "        IMAGE_RESIZE_MODE = \"pad64\"\n",
    "        # Non-max suppression threshold to filter RPN proposals.\n",
    "        # You can increase this during training to generate more propsals.\n",
    "        RPN_NMS_THRESHOLD = 0.7\n",
    "        # define the folder path to data for prediction\n",
    "        global data_dir\n",
    "        all_files = []\n",
    "        sub_directory = []\n",
    "        for root, dirs, files in os.walk(data_dir):\n",
    "            for file in files:\n",
    "                relativePath = os.path.relpath(root, data_dir)\n",
    "                if relativePath == \".\":\n",
    "                    relativePath = \"\"\n",
    "                all_files.append(\n",
    "                    (relativePath.count(os.path.sep), relativePath, file))\n",
    "        all_files.sort(reverse=True)\n",
    "        for (count, folder), files in groupby(all_files, itemgetter(0, 1)):\n",
    "            sub_directory.append(folder)\n",
    "# dataset = ImageDataset()\n",
    "# dataset.load_images(\"/home/ec2-user/SageMaker/images/\")\n",
    "# dataset.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_instance_json = \"/home/ec2-user/SageMaker/input_jsons/inputs.json\"\n",
    "config=CellInferenceConfig()\n",
    "model = modellib.MaskRCNN(mode=\"inference\", config=config, model_dir=data_dir)\n",
    "\n",
    "class ImageDataset(utils.Dataset):\n",
    "    def load_images(self, dataset_dir):\n",
    "        \"\"\"\n",
    "        Loads dataset images.\n",
    "        :param dataset_dir: string, path to dataset directory.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.add_class(\"cell\", 1, \"cell\")\n",
    "\n",
    "        image_ids = [fn for fn in os.listdir(dataset_dir)\n",
    "                     if any(fn.endswith(ext) for ext in ['tif', \"png\"])]\n",
    "\n",
    "        for image_id in image_ids:\n",
    "            self.add_image(\n",
    "                'cell',\n",
    "                image_id=os.path.splitext(image_id)[0],\n",
    "                path=os.path.join(dataset_dir, image_id)\n",
    "            )\n",
    "dataset = ImageDataset()\n",
    "dataset.load_images(data_dir)\n",
    "dataset.prepare()\n",
    "with open(predict_instance_json, \"w+\") as fp:\n",
    "    for image_id in dataset.image_ids:\n",
    "        image = dataset.load_image(image_id)\n",
    "        active_class_ids = np.zeros([dataset.num_classes], dtype=np.int32)\n",
    "        source_class_ids = dataset.source_class_ids[dataset.image_info[image_id][\"source\"]]\n",
    "        active_class_ids[source_class_ids] = 1\n",
    "        img, window, scale, padding, crop = utils.resize_image(\n",
    "                image,\n",
    "                min_dim=config.IMAGE_MIN_DIM,\n",
    "                min_scale=config.IMAGE_MIN_SCALE,\n",
    "                max_dim=config.IMAGE_MAX_DIM,\n",
    "                mode=config.IMAGE_RESIZE_MODE)\n",
    "        meta = np.array(\n",
    "            [image_id] +                  # size=1\n",
    "            list(image.shape) +           # size=3\n",
    "            list(img.shape) +             # size=3\n",
    "            list(window) +                # size=4 (y1, x1, y2, x2) in image cooredinates\n",
    "            [scale] +                     # size=1\n",
    "            list(active_class_ids)        # size=num_classes\n",
    "            )\n",
    "        anchors = model.get_anchors(image.shape)\n",
    "        json_data = {'input_image':image.tolist(),\n",
    "                     'input_image_meta':meta.tolist(),\n",
    "                     'input_anchors':anchors.tolist()} #,'key':int(image_id)\n",
    "#         jline = json.dumps(json_data) + \"\\n\"\n",
    "#         fp.write(jline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModelError",
     "evalue": "An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from model with message \"{ \"error\": \"Attempting to use uninitialized value rpn_class_raw/bias\\n\\t [[{{node rpn_class_raw/bias/read}}]]\" }\". See https://eu-west-1.console.aws.amazon.com/cloudwatch/home?region=eu-west-1#logEventViewer:group=/aws/sagemaker/Endpoints/tensorflow-inference-2019-07-09-11-32-10-037 in account 102554356212 for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModelError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-626011f26b37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/sagemaker/predictor.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, initial_args)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mrequest_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_request_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_runtime_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mrequest_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    356\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    659\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModelError\u001b[0m: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from model with message \"{ \"error\": \"Attempting to use uninitialized value rpn_class_raw/bias\\n\\t [[{{node rpn_class_raw/bias/read}}]]\" }\". See https://eu-west-1.console.aws.amazon.com/cloudwatch/home?region=eu-west-1#logEventViewer:group=/aws/sagemaker/Endpoints/tensorflow-inference-2019-07-09-11-32-10-037 in account 102554356212 for more information."
     ]
    }
   ],
   "source": [
    "predictor.predict(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
