{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pip in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (19.1.1)\n",
      "Requirement already up-to-date: pip in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (19.1.1)\n",
      "Requirement already satisfied: tifffile in /home/ec2-user/.local/lib/python3.6/site-packages (2019.7.2)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from tifffile) (1.14.5)\n",
      "Requirement already satisfied: tifffile in /home/ec2-user/.local/lib/python3.6/site-packages (2019.7.2)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from tifffile) (1.14.5)\n",
      "Requirement already satisfied: tifffile in /home/ec2-user/.local/lib/python3.6/site-packages (2019.7.2)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from tifffile) (1.14.5)\n",
      "Requirement already satisfied: tifffile in /home/ec2-user/.local/lib/python3.6/site-packages (2019.7.2)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from tifffile) (1.14.5)\n",
      "Requirement already satisfied: tifffile in /usr/local/lib/python3.6/site-packages (2019.7.2)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /usr/lib64/python3.6/dist-packages (from tifffile) (1.15.4)\n",
      "\u001b[33mYou are using pip version 19.0.2, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip3 install --upgrade pip\n",
    "!pip install tifffile\n",
    "!pip install tifffile --user\n",
    "!pip3 install tifffile\n",
    "!pip3 install tifffile --user\n",
    "!sudo python3 -m pip install tifffile\n",
    "import sys\n",
    "sys.path.append('/home/ec2-user/.local/bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::102554356212:role/service-role/AmazonSageMaker-ExecutionRole-20181129T100657\n",
      "CPU times: user 948 ms, sys: 124 ms, total: 1.07 s\n",
      "Wall time: 1.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from IPython.core.debugger import set_trace\n",
    "role = get_execution_role()\n",
    "print(role)\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-eu-west-1-102554356212\n"
     ]
    }
   ],
   "source": [
    "bucket = sess.default_bucket()  \n",
    "prefix = 'fresh_train_trial'\n",
    "print(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685385470294.dkr.ecr.eu-west-1.amazonaws.com/semantic-segmentation:latest\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "training_image = get_image_uri(sess.boto_region_name, 'semantic-segmentation', repo_version=\"latest\")\n",
    "print (training_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Create directory structure mimicing the s3 bucket where data is to be dumped.\n",
    "lf8 = '/home/ec2-user/SageMaker/raw_data/lior_figure_eight'\n",
    "os.makedirs(os.path.join(lf8,'train'), exist_ok=True)\n",
    "os.makedirs(os.path.join(lf8,'validation'), exist_ok=True)\n",
    "os.makedirs(os.path.join(lf8,'train_annotation'), exist_ok=True)\n",
    "os.makedirs(os.path.join(lf8,'validation_annotation'), exist_ok=True)\n",
    "\n",
    "train_channel = prefix + '/train'\n",
    "validation_channel = prefix + '/validation'\n",
    "train_annotation_channel = prefix + '/train_annotation'\n",
    "validation_annotation_channel = prefix + '/validation_annotation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import boto3\n",
    "import numpy as np\n",
    "from skimage import util \n",
    "from skimage.util import img_as_ubyte\n",
    "from skimage import exposure\n",
    "from skimage.io import imread as pngread\n",
    "from skimage.io import imsave as pngsave\n",
    "from tifffile import imsave,imread\n",
    "# gcspath = \"gs://segproj/PythonScripts/train/\"\n",
    "\n",
    "\n",
    "# Copy tif from s3 to local sagemaker\n",
    "files = []\n",
    "s3 = boto3.resource('s3')\n",
    "s3_resource = boto3.resource('s3')\n",
    "s3meadata = s3_resource.Bucket(name='meadata')\n",
    "num_training_samples = 0\n",
    "for obj in s3meadata.objects.all():\n",
    "    if 'tif' in obj.key:\n",
    "        num_training_samples = num_training_samples+1\n",
    "        file = obj.key\n",
    "        if 'segproj/training_data/train/' in obj.key:\n",
    "            s3.meta.client.download_file('meadata', file, '/tmp/raw.tif')\n",
    "            inverted_img = util.invert(imread('/tmp/raw.tif'))\n",
    "            jpgpath = '/tmp/'+file.split('/')[-2]+'_'+'raw.jpg'\n",
    "            image = exposure.rescale_intensity(inverted_img, out_range=(0, 2**31 - 1))\n",
    "            imsave(jpgpath,img_as_ubyte(image))\n",
    "#             set_trace()\n",
    "            sess.upload_data(path=jpgpath, bucket=bucket, key_prefix=train_channel)\n",
    "        elif 'segproj/training_data/val/' in obj.key:\n",
    "            s3.meta.client.download_file('meadata', file, '/tmp/raw.tif')\n",
    "            inverted_img = util.invert(imread('/tmp/raw.tif'))\n",
    "            jpgpath = '/tmp/'+file.split('/')[-2]+'_'+'raw.jpg'\n",
    "            image = exposure.rescale_intensity(inverted_img, out_range=(0, 2**31 - 1))\n",
    "            imsave(jpgpath,img_as_ubyte(image))\n",
    "            sess.upload_data(path=jpgpath, bucket=bucket, key_prefix=validation_channel)\n",
    "    elif 'instances_ids.png' in obj.key:\n",
    "        file = obj.key\n",
    "        if 'segproj/training_data/train/' in obj.key:\n",
    "            s3.meta.client.download_file('meadata', file, '/tmp/instances_ids.png')\n",
    "            pngpath = '/tmp/'+file.split('/')[-2]+'_'+'raw.png'\n",
    "            im = pngread('/tmp/instances_ids.png')\n",
    "            im[im > 0] = 1  #otherwise the trainer will think each cell number is a seperate category\n",
    "#             im[im==0]=255\n",
    "            pngsave(pngpath,np.uint8(im), check_contrast=False)\n",
    "            sess.upload_data(path=pngpath, bucket=bucket, key_prefix=train_annotation_channel)\n",
    "        elif 'segproj/training_data/val/' in obj.key:\n",
    "            s3.meta.client.download_file('meadata', file, '/tmp/instances_ids.png')\n",
    "            pngpath = '/tmp/'+file.split('/')[-2]+'_'+'raw.png'\n",
    "            im = pngread('/tmp/instances_ids.png')\n",
    "            im[im > 0] = 1  #otherwise the trainer will think each cell number is a seperate category\n",
    "#             im[im==0]=255\n",
    "            pngsave(pngpath,np.uint8(im), check_contrast=False)\n",
    "            sess.upload_data(path=pngpath, bucket=bucket, key_prefix=validation_annotation_channel)\n",
    "print(num_training_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from skimage.util import img_as_ubyte\n",
    "# from skimage import exposure\n",
    "# image = exposure.rescale_intensity(inverted_img, out_range=(0, 2**31 - 1))\n",
    "# plt.imshow(img_as_ubyte(im))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now move our prepared datset to the S3 bucket that we decided to use in this notebook earlier. Notice the following directory structure that is used.\n",
    "\n",
    "```bash\n",
    "root \n",
    "|-train/\n",
    "|-train_annotation/\n",
    "|-validation/\n",
    "|-validation_annotation/\n",
    "\n",
    "```\n",
    "Notice also that all the images in the `_annotation` directory are all indexed PNG files. This implies that the metadata (color mapping modes) of the files contain information on how to map the indices to colors and vice versa. Having an indexed PNG is an advantage as the images will be rendered by image viewers as color images, but the image themsevels only contain integers. The integers are also within `[0, 1 ... c-1, 255]`  for a `c` class segmentation problem, with `255` as 'hole' or 'ignore' class. We allow any mode that is a [recognized standard](https://pillow.readthedocs.io/en/3.0.x/handbook/concepts.html#concept-modes) as long as they are read as integers.\n",
    "\n",
    "While we recommend the format with default color mapping modes such as PASCAL, we also allow the customers to specify their own label maps. Refer to the [documentation](Permalink-to-label-map-documentation-section) for more details. The label map for the PASCAL VOC dataset, is the default (which we use incase no label maps are provided): \n",
    "```json\n",
    "{\n",
    "    \"scale\": 1\n",
    "}```\n",
    "This essentially tells us to simply use the images as read as integers as labels directly. Since we are using PASCAL dataset, let us create (recreate the default just for demonstration) label map for training channel and let the algorithm use the default (which is exactly the same for the validation channel). If `label_map` is used, please pass it to the label_map channel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "label_map = { \"scale\": 1 }\n",
    "with open('train_label_map.json', 'w') as lm_fname:\n",
    "    json.dump(label_map, lm_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-eu-west-1-102554356212/fresh_train_trial/output\n"
     ]
    }
   ],
   "source": [
    "s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "print(s3_output_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Now that we are done with all the setup that is needed, we are ready to train our segmentation algorithm. To begin, let us create a ``sageMaker.estimator.Estimator`` object. This estimator will launch the training job. Let us name our training job as `ss-notebook-demo`. Let us also use a nice-and-fast GPU instance (`ml.p3.2xlarge`) to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the sagemaker estimator object.\n",
    "ss_model = sagemaker.estimator.Estimator(training_image,\n",
    "                                         role, \n",
    "                                         train_instance_count = 1, \n",
    "                                         train_instance_type = 'ml.p3.16xlarge',\n",
    "                                         train_volume_size = 50,\n",
    "                                         train_max_run = 360000,\n",
    "                                         output_path = s3_output_location,\n",
    "                                         base_job_name = 'fresh-train-trial',\n",
    "                                         sagemaker_session = sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup hyperparameters \n",
    "ss_model.set_hyperparameters(backbone='resnet-50', # This is the encoder. Other option is resnet-50\n",
    "                             algorithm='deeplab', # This is the decoder. Other option is 'psp' and 'deeplab'                             \n",
    "                             use_pretrained_model='False', # Use the pre-trained model.\n",
    "                             crop_size=240, # Size of image random crop.                             \n",
    "                             num_classes=2, # Pascal has 21 classes. This is a mandatory parameter.\n",
    "                             epochs=500, # Number of epochs to run.\n",
    "                             learning_rate=0.0001,                             \n",
    "                             optimizer='rmsprop', # Other options include 'adam', 'rmsprop', 'nag', 'adagrad'.\n",
    "                             lr_scheduler='poly', # Other options include 'cosine' and 'step'.                           \n",
    "                             mini_batch_size=2, # Setup some mini batch size.\n",
    "                             validation_mini_batch_size=2,\n",
    "                             early_stopping=True, # Turn on early stopping. If OFF, other early stopping parameters are ignored.\n",
    "                             early_stopping_patience=2, # Tolerate these many epochs if the mIoU doens't increase.\n",
    "                             early_stopping_min_epochs=10, # No matter what, run these many number of epochs.                             \n",
    "                             num_training_samples=num_training_samples) # This is a mandatory parameter, 1464 in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the hyperparameters are setup, let us prepare the handshake between our data channels and the algorithm. To do this, we need to create the `sagemaker.session.s3_input` objects from our data channels. These objects are then put in a simple dictionary, which the algorithm uses to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create full bucket names\n",
    "s3_train_data = 's3://{}/{}'.format(bucket, train_channel)\n",
    "s3_validation_data = 's3://{}/{}'.format(bucket, validation_channel)\n",
    "s3_train_annotation = 's3://{}/{}'.format(bucket, train_annotation_channel)\n",
    "s3_validation_annotation = 's3://{}/{}'.format(bucket, validation_annotation_channel)\n",
    "\n",
    "distribution = 'FullyReplicated'\n",
    "# Create sagemaker s3_input objects\n",
    "train_data = sagemaker.session.s3_input(s3_train_data, distribution=distribution, \n",
    "                                        content_type='image/jpeg', s3_data_type='S3Prefix')\n",
    "validation_data = sagemaker.session.s3_input(s3_validation_data, distribution=distribution, \n",
    "                                        content_type='image/jpeg', s3_data_type='S3Prefix')\n",
    "train_annotation = sagemaker.session.s3_input(s3_train_annotation, distribution=distribution, \n",
    "                                        content_type='image/png', s3_data_type='S3Prefix')\n",
    "validation_annotation = sagemaker.session.s3_input(s3_validation_annotation, distribution=distribution, \n",
    "                                        content_type='image/png', s3_data_type='S3Prefix')\n",
    "\n",
    "data_channels = {'train': train_data, \n",
    "                 'validation': validation_data,\n",
    "                 'train_annotation': train_annotation, \n",
    "                 'validation_annotation':validation_annotation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "ss_model.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_predictor = ss_model.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us convert the image to bytearray before we supply it to our endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "\n",
    "filename = \"/home/ec2-user/SageMaker/images/190128_LD2_cleaned_ver1.tif\"\n",
    "# filename = \"/home/ec2-user/SageMaker/images/set10_raw.jpg\"\n",
    "im = imread(filename)\n",
    "num = int(''.join(filter(str.isdigit, str(im.dtype)))) - 1\n",
    "im = img_as_ubyte(exposure.rescale_intensity(im, out_range=(0, 2**num - 1)))\n",
    "im = img_as_ubyte(exposure.rescale_intensity(im, out_range=(0, 2**num - 1)))\n",
    "pngsave(filename.replace('tif','jpg'), im)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.imshow(im)\n",
    "plt.axis('off')\n",
    "with open(filename, 'rb') as image:\n",
    "    img = image.read()\n",
    "    img = bytearray(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "ss_predictor.content_type = 'image/jpeg'\n",
    "ss_predictor.accept = 'image/png'\n",
    "return_img = ss_predictor.predict(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us display the segmentation mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import io\n",
    "\n",
    "num_classes = 2\n",
    "mask = np.array(Image.open(io.BytesIO(return_img)))\n",
    "plt.imshow(mask)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the Endpoint\n",
    "Having an endpoint running will incur some costs. Therefore as a clean-up job, we should delete the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(ss_predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
