{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --upgrade pip\n",
    "!pip3 install --upgrade pip\n",
    "!pip install opencv-rolling-ball\n",
    "\n",
    "import sys\n",
    "import sagemaker\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import io\n",
    "import boto3\n",
    "import numpy as np\n",
    "from skimage import util \n",
    "from skimage.util import img_as_ubyte\n",
    "from skimage import exposure\n",
    "from skimage.io import imread as pngread\n",
    "from skimage.io import imsave as pngsave\n",
    "import cv2\n",
    "from rolling_ball_filter import rolling_ball_filter\n",
    "import random\n",
    "import threading\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from skimage import color\n",
    "from sagemaker import get_execution_role\n",
    "from IPython.core.debugger import set_trace\n",
    "from processfiles import *\n",
    "role = get_execution_role()\n",
    "print(role)\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "training_image = get_image_uri(sess.boto_region_name, 'semantic-segmentation', repo_version=\"latest\")\n",
    "print (training_image)\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "s3_resource = boto3.resource('s3')\n",
    "s3meadata = s3_resource.Bucket(name='meadata')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Run process functions (raw and filtered versions of fig8 and liorP)\n",
    "def procfilepar(key):\n",
    "    proccessliorpreprocfiles(key)\n",
    "    proccessliorfiles(key)\n",
    "    proccessfigure8files(key)\n",
    "    proccessfig8preprocfiles(key)\n",
    "    proccessusiigacifiles(key)\n",
    "    proccesshelafiles(key)\n",
    "    \n",
    "keys = [obj.key for obj in s3meadata.objects.all()]\n",
    "for key in keys:\n",
    "    t = threading.Thread(target = procfilepar, args=(key,)).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crop dataset images around labeled areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [obj.key for obj in s3_resource.Bucket(name=bucket).objects.all() if ('jpg' in obj.key and prefix in obj.key)]\n",
    "for key in keys:\n",
    "     t = threading.Thread(target = performcrop, args=(key,)).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete all files without a matching image-annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removeunmatched()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove samples with few segmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "train_channel = prefix + '/train'\n",
    "validation_channel = prefix + '/validation'\n",
    "train_annotation_channel = prefix + '/train_annotation'\n",
    "validation_annotation_channel = prefix + '/validation_annotation'\n",
    "keys = [obj.key for obj in s3_resource.Bucket(name=bucket).objects.all() if ('png' in obj.key and prefix in obj.key)]\n",
    "segs = []\n",
    "empties = []\n",
    "for key in keys:\n",
    "    masksavepath = \"/tmp/\"+key.split('/')[-1]\n",
    "    s3.meta.client.download_file(bucket, key , masksavepath)\n",
    "    mask = cv2.imread(masksavepath)\n",
    "    segs.append([np.sum(mask==1)])\n",
    "    empties.append([np.sum(mask==0)])\n",
    "\n",
    "ratio = ((np.asarray(segs)/np.asarray(empties))*100).ravel()\n",
    "thresh = np.round(np.mean(ratio)-np.std(ratio))\n",
    "plt.hist(ratio)\n",
    "plt.show()\n",
    "df = pd.DataFrame({'key':keys, 'ratio':ratio,'empty':ratio<thresh})\n",
    "removesamples = df['key'].loc[np.where(df['empty'].values)].values\n",
    "for removeme in removesamples:\n",
    "    boto3.client('s3').delete_object(Bucket = bucket, Key = removeme)\n",
    "    boto3.client('s3').delete_object(Bucket = bucket, Key = removeme.replace('_annotation/','/').replace('png','jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "label_map = { \"scale\": 1 }\n",
    "with open('train_label_map.json', 'w') as lm_fname:\n",
    "    json.dump(label_map, lm_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "print(s3_output_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the sagemaker estimator object.\n",
    "ss_model = sagemaker.estimator.Estimator(training_image,\n",
    "                                         role, \n",
    "                                         train_instance_count = 1, \n",
    "                                         train_instance_type = 'ml.p3.16xlarge',\n",
    "                                         train_volume_size = 300, # size in gb on s3 to reserve\n",
    "                                         train_max_run = 360000,\n",
    "                                         output_path = s3_output_location,\n",
    "                                         base_job_name = 'fresh-train-trial',\n",
    "                                         sagemaker_session = sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup hyperparameters \n",
    "import boto3\n",
    "s3traindata = boto3.resource('s3').Bucket(name=bucket)\n",
    "numtrain = len([obj.key for obj in s3traindata.objects.all() if ('train/' in obj.key and 'jpg' in obj.key)])\n",
    "ss_model.set_hyperparameters(backbone='resnet-101', # This is the encoder. Other option is resnet-50\n",
    "                             algorithm='deeplab', # This is the decoder. Other option is 'psp' and 'deeplab'                             \n",
    "                             use_pretrained_model='False', # Use the pre-trained model.\n",
    "                             crop_size=412, # Size of image random crop.                             \n",
    "                             num_classes=2, # Background + cell \n",
    "                             epochs=1000, # Number of epochs to run.\n",
    "                             learning_rate=0.003037052721870563, momentum = 0.6133596510181524, weight_decay = 0.0001560844683426084,                           \n",
    "                             optimizer='adagrad', # Other options include 'adam', 'rmsprop', 'nag', 'adagrad'.\n",
    "                             lr_scheduler='poly', # Other options include 'cosine' and 'step'.                           \n",
    "                             mini_batch_size=35, # Setup some mini batch size.\n",
    "                             validation_mini_batch_size=16, #try larger batch sizes maybe? \n",
    "                             early_stopping=True, # Turn on early stopping. If OFF, other early stopping parameters are ignored.\n",
    "                             early_stopping_patience=50, # Tolerate these many epochs if the mIoU doens't increase.\n",
    "                             early_stopping_min_epochs=25, # No matter what, run these many number of epochs.                             \n",
    "                             num_training_samples=numtrain) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup data inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create full bucket names\n",
    "s3_train_data = 's3://{}/{}'.format(bucket, train_channel)\n",
    "s3_validation_data = 's3://{}/{}'.format(bucket, validation_channel)\n",
    "s3_train_annotation = 's3://{}/{}'.format(bucket, train_annotation_channel)\n",
    "s3_validation_annotation = 's3://{}/{}'.format(bucket, validation_annotation_channel)\n",
    "\n",
    "distribution = 'FullyReplicated'\n",
    "# Create sagemaker s3_input objects\n",
    "train_data = sagemaker.session.s3_input(s3_train_data, distribution=distribution, \n",
    "                                        content_type='image/jpeg', s3_data_type='S3Prefix')\n",
    "validation_data = sagemaker.session.s3_input(s3_validation_data, distribution=distribution, \n",
    "                                        content_type='image/jpeg', s3_data_type='S3Prefix')\n",
    "train_annotation = sagemaker.session.s3_input(s3_train_annotation, distribution=distribution, \n",
    "                                        content_type='image/png', s3_data_type='S3Prefix')\n",
    "validation_annotation = sagemaker.session.s3_input(s3_validation_annotation, distribution=distribution, \n",
    "                                        content_type='image/png', s3_data_type='S3Prefix')\n",
    "\n",
    "data_channels = {'train': train_data, \n",
    "                 'validation': validation_data,\n",
    "                 'train_annotation': train_annotation, \n",
    "                 'validation_annotation':validation_annotation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit model and deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "ss_model.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the model to an endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_predictor = ss_model.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load an image for segmenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(64,64))\n",
    "# images/liorp_181106_2_raw.jpg\n",
    "filename = \"/home/ec2-user/SageMaker/itzik_images_cropped/_B37-543-2_07_ver2.tif\"\n",
    "# filename = \"/home/ec2-user/SageMaker/images/190221_LV_ver2.tif\"\n",
    "im = cv2.imread(filename)\n",
    "# selem = disk(60)\n",
    "# inimage = rank.equalize(inimage, selem=selem)  \n",
    "# im = clahe.apply(im)\n",
    "# im,_ = rolling_ball_filter(im, ball_radius = 20, spacing = 1, top=False)\n",
    "im =  cv2.resize(im, (1024,1024), interpolation = cv2.INTER_AREA)\n",
    "num = int(''.join(filter(str.isdigit, str(im.dtype)))) - 1\n",
    "im = img_as_ubyte(exposure.rescale_intensity(im, out_range=(0, 2**num - 1)))\n",
    "pngsave(filename.replace('tif','jpg'), im)\n",
    "\n",
    "with open(filename.replace('tif','jpg'), 'rb') as image:\n",
    "    img = image.read()    \n",
    "    img = bytearray(img)\n",
    "\n",
    "fig1 = plt.figure(figsize=(10, 10)) # create a figure with the default size \n",
    "ax1 = fig1.add_subplot(1,1,1) \n",
    "ax1.imshow(Image.open(io.BytesIO(img)), interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "ss_predictor.content_type = 'image/jpeg'\n",
    "ss_predictor.accept = 'image/png'\n",
    "return_img = ss_predictor.predict(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us display the segmentation mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import io\n",
    "from scipy import ndimage as ndi\n",
    "from skimage.feature import peak_local_max\n",
    "from skimage.segmentation import watershed\n",
    "import seaborn as sns\n",
    "from skimage.color import label2rgb\n",
    "\n",
    "num_classes = 2\n",
    "mask = np.array(Image.open(io.BytesIO(return_img)))\n",
    "distance = ndi.distance_transform_edt(mask)\n",
    "local_maxi = peak_local_max(distance, labels=mask, footprint=np.ones((3, 3)), indices=False)\n",
    "markers = ndi.label(local_maxi)[0]\n",
    "labels = watershed(-distance, markers, mask=mask)\n",
    "pngsave('/home/ec2-user/SageMaker/testresult_mask.tif', mask)\n",
    "\n",
    "fig1 = plt.figure(figsize=(20, 20)) # create a figure with the default size \n",
    "ax1 = fig1.add_subplot(2,2,1) \n",
    "result = label2rgb(label = labels, image = exposure.rescale_intensity(im.astype(np.float), out_range=(-1, 1)))\n",
    "ax1.imshow(result)\n",
    "ax2 = fig1.add_subplot(2,2,2) \n",
    "ax2.imshow(labels)\n",
    "plt.show()\n",
    "np.unique(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(ss_predictor.endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run inference on deployed model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessed submitted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createmultipleinputs(inputpath):\n",
    "    # rename __orig\n",
    "    # create __720_orig\n",
    "    # create __1024_orig\n",
    "    # create __720_prepoc\n",
    "    # create __1024_prepoc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run batch job from a saved models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batch Job\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from IPython.core.debugger import set_trace\n",
    "role = get_execution_role()\n",
    "print(role)\n",
    "sess = sagemaker.Session()\n",
    "# model_id = \"fresh-train-trial-2019-07-28-08-49-49-994\"\n",
    "# model_id = \"semantic-segmentatio-190726-1931-032-e7d26e04\"\n",
    "\n",
    "def runbatch(model_id):\n",
    "    env = {'SAGEMAKER_MODEL_SERVER_TIMEOUT' : '3600' }\n",
    "    s3 = boto3.resource('s3')\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    s3results = s3_resource.Bucket(name='sagemaker-eu-west-1-102554356212')\n",
    "    removesamples = [obj.key for obj in s3results.objects.all() if (\"results_\"+model_id in obj.key and (\"out\" in obj.key or \"masks\" in obj.key))]\n",
    "    for removeme in removesamples:\n",
    "        boto3.client('s3').delete_object(Bucket = bucket, Key = removeme)\n",
    "\n",
    "    transform_job = sagemaker.transformer.Transformer(\n",
    "        model_name = model_id, \n",
    "        instance_count = 1,\n",
    "        instance_type = 'ml.p3.2xlarge',\n",
    "        strategy = 'SingleRecord',\n",
    "        assemble_with = 'None',\n",
    "        output_path = \"s3://sagemaker-eu-west-1-102554356212/results_\"+model_id,\n",
    "        base_transform_job_name='inference-pipelines-batch',\n",
    "        sagemaker_session=sess,\n",
    "        accept = 'image/png',\n",
    "        env = env)\n",
    "    transform_job.transform(data = 's3://sagemaker-eu-west-1-102554356212/submissions/' , \n",
    "                            content_type = 'image/jpeg', \n",
    "                            split_type = None)\n",
    "\n",
    "runbatch(\"semantic-segmentatio-190726-1931-032-e7d26e04\")\n",
    "runbatch(\"fresh-train-trial-2019-07-28-08-49-49-994\")\n",
    "# transform_job.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read batch processed results and export mask back to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Download data from batch job\n",
    "import boto3\n",
    "import mxnet as mx\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import io\n",
    "import os\n",
    "from skimage.io import imread as pngread\n",
    "from skimage.io import imsave as pngsave\n",
    "\n",
    "def batch2masks(model_id):\n",
    "    s3 = boto3.resource('s3')\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    s3results = s3_resource.Bucket(name='sagemaker-eu-west-1-102554356212')\n",
    "    keys = [obj.key for obj in s3results.objects.all()]\n",
    "    os.makedirs('/tmp/results/', exist_ok=True)\n",
    "    for s3_object in keys:\n",
    "        if not s3_object.endswith(\"/\") and \"results_\"+model_id+\"/\" in s3_object and \"out\" in s3_object:\n",
    "                s3.meta.client.download_file('sagemaker-eu-west-1-102554356212', s3_object, '/tmp/tempfile.out')\n",
    "                num_classes = 2\n",
    "                with open('/tmp/tempfile.out', 'rb') as image:\n",
    "                    img = image.read()    \n",
    "                    img = bytearray(img)\n",
    "                    mask = np.array(Image.open(io.BytesIO(img)))\n",
    "                    pngsave('/tmp/results/'+'.'.join(s3_object.split('/')[-1].split('.')[:-1]), mask)\n",
    "    os.system(\"aws s3 sync '/tmp/results/' 's3://sagemaker-eu-west-1-102554356212/results_{}/masks/' \".format(model_id))\n",
    "batch2masks(\"fresh-train-trial-2019-07-28-08-49-49-994\")\n",
    "batch2masks(\"semantic-segmentatio-190726-1931-032-e7d26e04\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge multiple masks from different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from processfiles import merge_multiple_detections\n",
    "from scipy import ndimage as ndi\n",
    "from skimage.feature import peak_local_max\n",
    "from skimage.segmentation import watershed\n",
    "from skimage.color import label2rgb\n",
    "def merge_two_masks(maskpaths):\n",
    "    masks = []\n",
    "    for mpath in maskpaths:\n",
    "        binarymask = pngread(mpath)\n",
    "        num_classes = 2\n",
    "        distance = ndi.distance_transform_edt(binarymask)\n",
    "        local_maxi = peak_local_max(distance, labels=binarymask, footprint=np.ones((3, 3)), indices=False)\n",
    "        markers = ndi.label(local_maxi)[0]\n",
    "        masks.append(watershed(-distance, markers, mask=binarymask))\n",
    "    mask = merge_multiple_detections(masks)\n",
    "    return(mask)\n",
    "\n",
    "def merge_masks(s3_object, model_ids):\n",
    "    os.makedirs('/tmp/results/merge/merged/', exist_ok=True)\n",
    "    outpaths=[]\n",
    "    ismodelresult = any([m for m in model_ids if m in s3_object])\n",
    "    if ismodelresult:\n",
    "        for model_id in model_ids:\n",
    "            if not s3_object.endswith(\"/\") and \"masks\" in s3_object:\n",
    "                    outpath = os.path.join('/tmp/results/merge/',model_id+'_'+s3_object.split('/')[-1])\n",
    "                    s3.meta.client.download_file('sagemaker-eu-west-1-102554356212', s3_object, outpath)\n",
    "                    outpaths.append(outpath)\n",
    "        if outpaths:            \n",
    "            mask = merge_two_masks(outpaths)\n",
    "            pngsave(os.path.join('/tmp/results/merge/merged/','merged_'+s3_object.split('/')[-1]), np.uint8(mask>0))\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "s3_resource = boto3.resource('s3')\n",
    "s3results = s3_resource.Bucket(name='sagemaker-eu-west-1-102554356212')\n",
    "keys = [obj.key for obj in s3results.objects.all()]\n",
    "for s3_object in keys:\n",
    "    t = threading.Thread(target = merge_masks, args=(s3_object,[\"fresh-train-trial-2019-07-28-08-49-49-994\",\"semantic-segmentatio-190726-1931-032-e7d26e04\"],)).start()\n",
    "!aws s3 sync '/tmp/results/merge/merged/' 's3://sagemaker-eu-west-1-102554356212/results_merged/masks/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge multiple masks from different inputs (different pre-processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "def merge_masks_diff_inputs(groupkeys):\n",
    "    os.makedirs('/tmp/results/merge/input_merge/', exist_ok=True)\n",
    "    for s3_object in groupkeys:\n",
    "        if not s3_object.endswith(\"/\") and \"masks\" in s3_object:\n",
    "                outpath = os.path.join('/tmp/results/'+s3_object.split('/')[-1])\n",
    "                s3.meta.client.download_file('sagemaker-eu-west-1-102554356212', s3_object, outpath)\n",
    "                outpaths.append(outpath)\n",
    "    if outpaths:            \n",
    "        binarymask = merge_two_masks(outpaths)\n",
    "        num_classes = 2\n",
    "        distance = ndi.distance_transform_edt(binarymask)\n",
    "        local_maxi = peak_local_max(distance, labels=binarymask, footprint=np.ones((3, 3)), indices=False)\n",
    "        markers = ndi.label(local_maxi)[0]\n",
    "        mask = watershed(-distance, markers, mask=binarymask)\n",
    "        pngsave(os.path.join('/tmp/results/merge/input_merge/','inputmerged_'+s3_object.split('/')[-1].split('__')[0]), np.uint8(mask>0))\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "s3_resource = boto3.resource('s3')\n",
    "s3results = s3_resource.Bucket(name='sagemaker-eu-west-1-102554356212')\n",
    "keys = [obj.key for obj in s3results.objects.all()]\n",
    "df = pd.DataFrame({'keys':keys,'orig_name':[k.split('/')[-1].split('__')[0] for k in keys]})\n",
    "originals = np.unique(df['orig_name'])\n",
    "for org in originals:\n",
    "    merge_masks_diff_inputs(groupkeys = df['keys'].loc(['orig_name'==org]).values)\n",
    "# !aws s3 sync '/tmp/results/merge/input_merged/' 's3://sagemaker-eu-west-1-102554356212/results_merged/input_merged_masks/'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
