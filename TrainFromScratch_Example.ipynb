{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --upgrade pip\n",
    "!pip3 install --upgrade pip\n",
    "!pip install opencv-rolling-ball\n",
    "!pip install tifffile\n",
    "!pip install tifffile --user\n",
    "!pip3 install tifffile\n",
    "!pip3 install tifffile --user\n",
    "!sudo python3 -m pip install tifffile\n",
    "import sys\n",
    "sys.path.append('/home/ec2-user/.local/bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::102554356212:role/service-role/AmazonSageMaker-ExecutionRole-20181129T100657\n",
      "CPU times: user 1.3 s, sys: 769 ms, total: 2.07 s\n",
      "Wall time: 7.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from IPython.core.debugger import set_trace\n",
    "role = get_execution_role()\n",
    "print(role)\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-eu-west-1-102554356212\n"
     ]
    }
   ],
   "source": [
    "bucket = sess.default_bucket()  \n",
    "prefix = 'fresh_train_trial'\n",
    "train_channel = prefix + '/train'\n",
    "validation_channel = prefix + '/validation'\n",
    "train_annotation_channel = prefix + '/train_annotation'\n",
    "validation_annotation_channel = prefix + '/validation_annotation'\n",
    "print(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685385470294.dkr.ecr.eu-west-1.amazonaws.com/semantic-segmentation:latest\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "training_image = get_image_uri(sess.boto_region_name, 'semantic-segmentation', repo_version=\"latest\")\n",
    "print (training_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Hela Cell Dataset Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/skimage/util/dtype.py:122: UserWarning: Possible precision loss when converting from uint16 to uint8\n",
      "  .format(dtypeobj_in, dtypeobj_out))\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/skimage/util/dtype.py:122: UserWarning: Possible precision loss when converting from float64 to int16\n",
      "  .format(dtypeobj_in, dtypeobj_out))\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/skimage/io/_io.py:132: UserWarning: /tmp/hela_t1_t012_raw.png is a low contrast image\n",
      "  warn('%s is a low contrast image' % fname)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/skimage/io/_io.py:132: UserWarning: /tmp/hela_t1_t013_raw.png is a low contrast image\n",
      "  warn('%s is a low contrast image' % fname)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/skimage/io/_io.py:132: UserWarning: /tmp/hela_t1_t014_raw.png is a low contrast image\n",
      "  warn('%s is a low contrast image' % fname)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/skimage/io/_io.py:132: UserWarning: /tmp/hela_t1_t015_raw.png is a low contrast image\n",
      "  warn('%s is a low contrast image' % fname)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/skimage/io/_io.py:132: UserWarning: /tmp/hela_t1_t020_raw.png is a low contrast image\n",
      "  warn('%s is a low contrast image' % fname)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/skimage/io/_io.py:132: UserWarning: /tmp/hela_t1_t021_raw.png is a low contrast image\n",
      "  warn('%s is a low contrast image' % fname)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/skimage/io/_io.py:132: UserWarning: /tmp/hela_t1_t022_raw.png is a low contrast image\n",
      "  warn('%s is a low contrast image' % fname)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/skimage/io/_io.py:132: UserWarning: /tmp/hela_t1_t023_raw.png is a low contrast image\n",
      "  warn('%s is a low contrast image' % fname)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/skimage/io/_io.py:132: UserWarning: /tmp/hela_t1_t025_raw.png is a low contrast image\n",
      "  warn('%s is a low contrast image' % fname)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/skimage/io/_io.py:132: UserWarning: /tmp/hela_t1_t029_raw.png is a low contrast image\n",
      "  warn('%s is a low contrast image' % fname)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/skimage/io/_io.py:132: UserWarning: /tmp/hela_t1_t038_raw.png is a low contrast image\n",
      "  warn('%s is a low contrast image' % fname)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/skimage/io/_io.py:132: UserWarning: /tmp/hela_t1_t039_raw.png is a low contrast image\n",
      "  warn('%s is a low contrast image' % fname)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/skimage/io/_io.py:132: UserWarning: /tmp/hela_t1_t040_raw.png is a low contrast image\n",
      "  warn('%s is a low contrast image' % fname)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/skimage/io/_io.py:132: UserWarning: /tmp/hela_t1_t044_raw.png is a low contrast image\n",
      "  warn('%s is a low contrast image' % fname)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/skimage/io/_io.py:132: UserWarning: /tmp/hela_t1_t050_raw.png is a low contrast image\n",
      "  warn('%s is a low contrast image' % fname)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/skimage/io/_io.py:132: UserWarning: /tmp/hela_t1_t051_raw.png is a low contrast image\n",
      "  warn('%s is a low contrast image' % fname)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/skimage/io/_io.py:132: UserWarning: /tmp/hela_t1_t052_raw.png is a low contrast image\n",
      "  warn('%s is a low contrast image' % fname)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/skimage/io/_io.py:132: UserWarning: /tmp/hela_t1_t053_raw.png is a low contrast image\n",
      "  warn('%s is a low contrast image' % fname)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/skimage/io/_io.py:132: UserWarning: /tmp/hela_t1_t054_raw.png is a low contrast image\n",
      "  warn('%s is a low contrast image' % fname)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/skimage/io/_io.py:132: UserWarning: /tmp/hela_t1_t055_raw.png is a low contrast image\n",
      "  warn('%s is a low contrast image' % fname)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/skimage/io/_io.py:132: UserWarning: /tmp/hela_t1_t062_raw.png is a low contrast image\n",
      "  warn('%s is a low contrast image' % fname)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/skimage/io/_io.py:132: UserWarning: /tmp/hela_t1_t076_raw.png is a low contrast image\n",
      "  warn('%s is a low contrast image' % fname)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/skimage/io/_io.py:132: UserWarning: /tmp/hela_t1_t077_raw.png is a low contrast image\n",
      "  warn('%s is a low contrast image' % fname)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/skimage/io/_io.py:132: UserWarning: /tmp/hela_t1_t078_raw.png is a low contrast image\n",
      "  warn('%s is a low contrast image' % fname)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/skimage/io/_io.py:132: UserWarning: /tmp/hela_t1_t079_raw.png is a low contrast image\n",
      "  warn('%s is a low contrast image' % fname)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/skimage/io/_io.py:132: UserWarning: /tmp/hela_t1_t080_raw.png is a low contrast image\n",
      "  warn('%s is a low contrast image' % fname)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/skimage/io/_io.py:132: UserWarning: /tmp/hela_t1_t081_raw.png is a low contrast image\n",
      "  warn('%s is a low contrast image' % fname)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/skimage/io/_io.py:132: UserWarning: /tmp/hela_t1_t088_raw.png is a low contrast image\n",
      "  warn('%s is a low contrast image' % fname)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/skimage/io/_io.py:132: UserWarning: /tmp/hela_t2_t023_raw.png is a low contrast image\n",
      "  warn('%s is a low contrast image' % fname)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/skimage/io/_io.py:132: UserWarning: /tmp/hela_t2_t035_raw.png is a low contrast image\n",
      "  warn('%s is a low contrast image' % fname)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/skimage/io/_io.py:132: UserWarning: /tmp/hela_t2_t036_raw.png is a low contrast image\n",
      "  warn('%s is a low contrast image' % fname)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/skimage/io/_io.py:132: UserWarning: /tmp/hela_t2_t067_raw.png is a low contrast image\n",
      "  warn('%s is a low contrast image' % fname)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/skimage/io/_io.py:132: UserWarning: /tmp/hela_t2_t075_raw.png is a low contrast image\n",
      "  warn('%s is a low contrast image' % fname)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/skimage/io/_io.py:132: UserWarning: /tmp/hela_t2_t078_raw.png is a low contrast image\n",
      "  warn('%s is a low contrast image' % fname)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/skimage/io/_io.py:132: UserWarning: /tmp/hela_t2_t079_raw.png is a low contrast image\n",
      "  warn('%s is a low contrast image' % fname)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/skimage/io/_io.py:132: UserWarning: /tmp/hela_t2_t087_raw.png is a low contrast image\n",
      "  warn('%s is a low contrast image' % fname)\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "#Hela cell dataset\n",
    "import boto3\n",
    "import numpy as np\n",
    "from skimage import util \n",
    "from skimage.util import img_as_ubyte\n",
    "from skimage import exposure,color, img_as_int\n",
    "from skimage.io import imread as pngread\n",
    "from skimage.io import imsave as pngsave\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from tifffile import imsave,imread\n",
    "import cv2\n",
    "from rolling_ball_filter import rolling_ball_filter\n",
    "import random\n",
    "import threading\n",
    "\n",
    "files = []\n",
    "s3 = boto3.resource('s3')\n",
    "s3_resource = boto3.resource('s3')\n",
    "s3meadata = s3_resource.Bucket(name='meadata')\n",
    "#remember to upload files\n",
    "def proccesshelafiles(f, s3 = s3, sess = sess, bucket = bucket, prefix = prefix):\n",
    "    train_channel = prefix + '/train'\n",
    "    validation_channel = prefix + '/validation'\n",
    "    train_annotation_channel = prefix + '/train_annotation'\n",
    "    validation_annotation_channel = prefix + '/validation_annotation'\n",
    "    if 'jpg' in f:\n",
    "        file = f\n",
    "        if 'segproj/hela_dataset_training_data/train/' in f:\n",
    "            jpgpath = '/tmp/'+'hela_'+file.split('/')[-1].split('.')[0]+'_'+'raw.jpg'\n",
    "            s3.meta.client.download_file('meadata', file, jpgpath)\n",
    "            inverted_img = pngread(jpgpath)\n",
    "            num = int(''.join(filter(str.isdigit, str(inverted_img.dtype)))) - 1\n",
    "            image = exposure.rescale_intensity(inverted_img, out_range=(0, 2**num - 1))\n",
    "            image =  cv2.resize(image, (1024,1024), interpolation = cv2.INTER_AREA) \n",
    "            pngsave(jpgpath,img_as_ubyte(image))\n",
    "            sess.upload_data(path=jpgpath, bucket=bucket, key_prefix=train_channel)\n",
    "        elif 'segproj/hela_dataset_training_data/val/' in f:\n",
    "            jpgpath = '/tmp/'+'hela_'+file.split('/')[-1].split('.')[0]+'_'+'raw.jpg'\n",
    "            s3.meta.client.download_file('meadata', file, jpgpath)\n",
    "            inverted_img = pngread(jpgpath)\n",
    "            num = int(''.join(filter(str.isdigit, str(inverted_img.dtype)))) - 1\n",
    "            image = exposure.rescale_intensity(inverted_img, out_range=(0, 2**num - 1))\n",
    "            image =  cv2.resize(image, (1024,1024), interpolation = cv2.INTER_AREA) \n",
    "            pngsave(jpgpath,img_as_ubyte(image))\n",
    "            sess.upload_data(path=jpgpath, bucket=bucket, key_prefix=validation_channel)\n",
    "    elif 'png' in f:\n",
    "        file = f\n",
    "        if 'segproj/hela_dataset_training_data/train_annotation/' in f:\n",
    "            pngpath = '/tmp/'+'hela_'+file.split('/')[-1].split('.')[0]+'_'+'raw.png'\n",
    "            s3.meta.client.download_file('meadata', file, pngpath)\n",
    "            im1 = pngread(pngpath)\n",
    "            num = int(''.join(filter(str.isdigit, str(im1.dtype)))) - 1\n",
    "            image = exposure.rescale_intensity(im1, out_range=(0, 2**num - 1))\n",
    "            image = img_as_ubyte(image)\n",
    "            im = mark_boundaries(image, im1, color = [0,0,0], outline_color = [0,0,0], mode='outer', background_label=0)\n",
    "            im2 = img_as_int(im)\n",
    "            im3 = np.zeros([im2.shape[0],im2.shape[1]])\n",
    "            im3 = im2[:,:,0]+im2[:,:,1]+im2[:,:,2]\n",
    "            im3 = np.uint8((im3>0))\n",
    "            im3 =  cv2.resize(im3, (1024,1024), interpolation = cv2.INTER_AREA)                 \n",
    "            pngsave(pngpath,im3, check_contrast=False)            \n",
    "            sess.upload_data(path=pngpath, bucket=bucket, key_prefix=train_annotation_channel)\n",
    "        elif 'segproj/hela_dataset_training_data/val_annotation/' in f:\n",
    "            pngpath = '/tmp/'+'hela_'+file.split('/')[-1].split('.')[0]+'_'+'raw.png'\n",
    "            s3.meta.client.download_file('meadata', file, pngpath)\n",
    "            im1 = pngread(pngpath)\n",
    "            num = int(''.join(filter(str.isdigit, str(im1.dtype)))) - 1\n",
    "            image = exposure.rescale_intensity(im1, out_range=(0, 2**num - 1))\n",
    "            image = img_as_ubyte(image)\n",
    "            im = mark_boundaries(image, im1, color = [0,0,0], outline_color = [0,0,0], mode='outer', background_label=0)\n",
    "            im2 = img_as_int(im)\n",
    "            im3 = np.zeros([im2.shape[0],im2.shape[1]])\n",
    "            im3 = im2[:,:,0]+im2[:,:,1]+im2[:,:,2]\n",
    "            im3 = np.uint8((im3>0))\n",
    "            im3 =  cv2.resize(im3, (1024,1024), interpolation = cv2.INTER_AREA)                 \n",
    "            pngsave(pngpath,im3, check_contrast=False)            \n",
    "            sess.upload_data(path=pngpath, bucket=bucket, key_prefix=validation_annotation_channel)\n",
    "\n",
    "keys = [obj.key for obj in s3meadata.objects.all()]\n",
    "for key in keys:\n",
    "    proccesshelafiles(key)\n",
    "#     t = threading.Thread(target = proccesshelafiles, args=(key,)).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Figure8 Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#figure8\n",
    "import boto3\n",
    "import numpy as np\n",
    "from skimage import util \n",
    "from skimage.util import img_as_ubyte\n",
    "from skimage import exposure,color, img_as_int\n",
    "from skimage.io import imread as pngread\n",
    "from skimage.io import imsave as pngsave\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from tifffile import imsave,imread\n",
    "import cv2\n",
    "from rolling_ball_filter import rolling_ball_filter\n",
    "import random\n",
    "import threading\n",
    "\n",
    "files = []\n",
    "s3 = boto3.resource('s3')\n",
    "s3_resource = boto3.resource('s3')\n",
    "s3meadata = s3_resource.Bucket(name='meadata')\n",
    "\n",
    "def proccessfigure8files(f, s3 = s3, sess = sess, bucket = bucket, prefix = prefix):\n",
    "    train_channel = prefix + '/train'\n",
    "    validation_channel = prefix + '/validation'\n",
    "    train_annotation_channel = prefix + '/train_annotation'\n",
    "    validation_annotation_channel = prefix + '/validation_annotation'\n",
    "    if 'tif' in f:\n",
    "        file = f\n",
    "        if 'segproj/training_data/train/' in f:\n",
    "            jpgpath = '/tmp/'+'fig8_'+file.split('/')[-2]+'_'+'raw.jpg'\n",
    "            s3.meta.client.download_file('meadata', file, jpgpath.replace('jpg','tif'))\n",
    "            inverted_img = imread(jpgpath.replace('jpg','tif'))\n",
    "            num = int(''.join(filter(str.isdigit, str(inverted_img.dtype)))) - 1\n",
    "            image = exposure.rescale_intensity(inverted_img, out_range=(2**num - 1, 0))\n",
    "#             image = util.invert(image)\n",
    "            imsave(jpgpath,img_as_ubyte(image))\n",
    "            sess.upload_data(path=jpgpath, bucket=bucket, key_prefix=train_channel)\n",
    "        elif 'segproj/training_data/val/' in f:\n",
    "            jpgpath = '/tmp/'+'fig8_'+file.split('/')[-2]+'_'+'raw.jpg'\n",
    "            s3.meta.client.download_file('meadata', file, jpgpath.replace('jpg','tif'))\n",
    "            inverted_img = imread(jpgpath.replace('jpg','tif'))\n",
    "            num = int(''.join(filter(str.isdigit, str(inverted_img.dtype)))) - 1\n",
    "            image = exposure.rescale_intensity(inverted_img, out_range=(2**num - 1, 0))\n",
    "#             image = util.invert(image)\n",
    "            imsave(jpgpath,img_as_ubyte(image))\n",
    "            sess.upload_data(path=jpgpath, bucket=bucket, key_prefix=validation_channel)\n",
    "    elif 'instances_ids.png' in f:\n",
    "        file = f\n",
    "        if 'segproj/training_data/train/' in f:\n",
    "            pngpath = '/tmp/'+'fig8_'+file.split('/')[-2]+'_'+'raw.png'\n",
    "            s3.meta.client.download_file('meadata', file, pngpath)\n",
    "            im1 = pngread(pngpath)\n",
    "            num = int(''.join(filter(str.isdigit, str(im1.dtype)))) - 1\n",
    "            image = exposure.rescale_intensity(im1, out_range=(0, 2**num - 1))\n",
    "            image = img_as_ubyte(image)\n",
    "            im = mark_boundaries(image, im1, color = [0,0,0], outline_color = [0,0,0], mode='outer', background_label=0)\n",
    "            im2 = img_as_int(im)\n",
    "            im3 = np.zeros([im2.shape[0],im2.shape[1]])\n",
    "            im3 = im2[:,:,0]+im2[:,:,1]+im2[:,:,2]\n",
    "            im3 = np.uint8((im3>0))\n",
    "            im3 =  cv2.resize(im3, (1024,1024), interpolation = cv2.INTER_AREA)                 \n",
    "            pngsave(pngpath,im3, check_contrast=False)            \n",
    "            sess.upload_data(path=pngpath, bucket=bucket, key_prefix=train_annotation_channel)\n",
    "        elif 'segproj/training_data/val/' in f:\n",
    "            pngpath = '/tmp/'+'fig8_'+file.split('/')[-2]+'_'+'raw.png'\n",
    "            s3.meta.client.download_file('meadata', file, pngpath)\n",
    "            im1 = pngread(pngpath)\n",
    "            num = int(''.join(filter(str.isdigit, str(im1.dtype)))) - 1\n",
    "            image = exposure.rescale_intensity(im1, out_range=(0, 2**num - 1))\n",
    "            image = img_as_ubyte(image)\n",
    "            im = mark_boundaries(image, im1, color = [0,0,0], outline_color = [0,0,0], mode='outer', background_label=0)\n",
    "            im2 = img_as_int(im)\n",
    "            im3 = np.zeros([im2.shape[0],im2.shape[1]])\n",
    "            im3 = im2[:,:,0]+im2[:,:,1]+im2[:,:,2]\n",
    "            im3 = np.uint8((im3>0))\n",
    "            im3 =  cv2.resize(im3, (1024,1024), interpolation = cv2.INTER_AREA)                 \n",
    "            pngsave(pngpath,im3, check_contrast=False)            \n",
    "            sess.upload_data(path=pngpath, bucket=bucket, key_prefix=validation_annotation_channel)\n",
    "\n",
    "keys = [obj.key for obj in s3meadata.objects.all()]\n",
    "for key in keys:\n",
    "    t = threading.Thread(target = proccessfigure8files, args=(key,)).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Usiigaci Ground Truth (phase contrast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#usiigaci\n",
    "import boto3\n",
    "import numpy as np\n",
    "from skimage import util \n",
    "from skimage.util import img_as_ubyte\n",
    "from skimage import exposure\n",
    "from skimage.io import imread as pngread\n",
    "from skimage.io import imsave as pngsave\n",
    "from tifffile import imsave,imread\n",
    "import cv2\n",
    "from rolling_ball_filter import rolling_ball_filter\n",
    "import random\n",
    "import threading\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from skimage import color\n",
    "files = []\n",
    "s3 = boto3.resource('s3')\n",
    "s3_resource = boto3.resource('s3')\n",
    "s3meadata = s3_resource.Bucket(name='meadata')\n",
    "\n",
    "def proccessusiigacifiles(f, s3 = s3, sess = sess, bucket = bucket, prefix = prefix):\n",
    "    train_channel = prefix + '/train'\n",
    "    validation_channel = prefix + '/validation'\n",
    "    train_annotation_channel = prefix + '/train_annotation'\n",
    "    validation_annotation_channel = prefix + '/validation_annotation'\n",
    "    if 'tif' in f:\n",
    "        file = f\n",
    "        if 'segproj/usiigaci_train_data/train/' in f:\n",
    "            jpgpath = '/tmp/'+'usiigaci_'+file.split('/')[-2]+'_'+'raw.jpg'\n",
    "            s3.meta.client.download_file('meadata', file, jpgpath.replace('jpg','tif'))\n",
    "            inverted_img = util.invert(imread(jpgpath.replace('jpg','tif')))\n",
    "            num = int(''.join(filter(str.isdigit, str(inverted_img.dtype)))) - 1\n",
    "            image = exposure.rescale_intensity(inverted_img, out_range=(0, 2**num - 1))\n",
    "            image =  cv2.resize(img_as_ubyte(image), (1024,1024), interpolation = cv2.INTER_AREA)\n",
    "#             image,_ = rolling_ball_filter(image, ball_radius = 20, spacing = 1, top=False)\n",
    "            imsave(jpgpath,image)\n",
    "            sess.upload_data(path=jpgpath, bucket=bucket, key_prefix=train_channel)\n",
    "        elif 'segproj/usiigaci_train_data/val/' in f:\n",
    "            jpgpath = '/tmp/'+'usiigaci_'+file.split('/')[-2]+'_'+'raw.jpg'\n",
    "            s3.meta.client.download_file('meadata', file, jpgpath.replace('jpg','tif'))\n",
    "            inverted_img = util.invert(imread(jpgpath.replace('jpg','tif')))\n",
    "            num = int(''.join(filter(str.isdigit, str(inverted_img.dtype)))) - 1\n",
    "            image = exposure.rescale_intensity(inverted_img, out_range=(0, 2**num - 1))\n",
    "            image =  cv2.resize(img_as_ubyte(image), (1024,1024), interpolation = cv2.INTER_AREA)\n",
    "#             image,_ = rolling_ball_filter(image, ball_radius = 20, spacing = 1, top=False)\n",
    "            imsave(jpgpath,image)\n",
    "            sess.upload_data(path=jpgpath, bucket=bucket, key_prefix=validation_channel)\n",
    "    elif 'instances_ids.png' in f:\n",
    "        file = f\n",
    "        if 'segproj/usiigaci_train_data/train/' in f:\n",
    "            pngpath = '/tmp/'+'usiigaci_'+file.split('/')[-2]+'_'+'raw.png'\n",
    "            s3.meta.client.download_file('meadata', file, pngpath)\n",
    "            im1 = pngread(pngpath)\n",
    "            num = int(''.join(filter(str.isdigit, str(im1.dtype)))) - 1\n",
    "            image = exposure.rescale_intensity(im1, out_range=(0, 2**num - 1))\n",
    "            image = img_as_ubyte(image)\n",
    "            im = mark_boundaries(image, im1, color = [0,0,0], outline_color = [0,0,0], mode='outer', background_label=0)\n",
    "            im2 = img_as_int(im)\n",
    "            im3 = np.zeros([im2.shape[0],im2.shape[1]])\n",
    "            im3 = im2[:,:,0]+im2[:,:,1]+im2[:,:,2]\n",
    "            im3 = np.uint8((im3>0))\n",
    "            im3 =  cv2.resize(im3, (1024,1024), interpolation = cv2.INTER_AREA)                 \n",
    "            pngsave(pngpath,im3, check_contrast=False)\n",
    "            sess.upload_data(path=pngpath, bucket=bucket, key_prefix=train_annotation_channel)\n",
    "        elif 'segproj/usiigaci_train_data/val/' in f:\n",
    "            pngpath = '/tmp/'+'usiigaci_'+file.split('/')[-2]+'_'+'raw.png'\n",
    "            s3.meta.client.download_file('meadata', file, pngpath)\n",
    "            im1 = pngread(pngpath)\n",
    "            num = int(''.join(filter(str.isdigit, str(im1.dtype)))) - 1\n",
    "            image = exposure.rescale_intensity(im1, out_range=(0, 2**num - 1))\n",
    "            image = img_as_ubyte(image)\n",
    "            im = mark_boundaries(image, im1, color = [0,0,0], outline_color = [0,0,0], mode='outer', background_label=0)\n",
    "            im2 = img_as_int(im)\n",
    "            im3 = np.zeros([im2.shape[0],im2.shape[1]])\n",
    "            im3 = im2[:,:,0]+im2[:,:,1]+im2[:,:,2]\n",
    "            im3 = np.uint8((im3>0))\n",
    "            im3 =  cv2.resize(im3, (1024,1024), interpolation = cv2.INTER_AREA)                 \n",
    "            pngsave(pngpath,im3, check_contrast=False)\n",
    "            sess.upload_data(path=pngpath, bucket=bucket, key_prefix=validation_annotation_channel)\n",
    "    \n",
    "            \n",
    "keys = [obj.key for obj in s3meadata.objects.all()]\n",
    "for key in keys:\n",
    "    t = threading.Thread(target = proccessusiigacifiles, args=(key,)).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Lior's Ground Truth (manually adjusted MaskRCNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "def proccessliorfiles(f, s3 = s3, sess = sess, bucket = bucket, prefix = prefix):\n",
    "    train_channel = prefix + '/train'\n",
    "    validation_channel = prefix + '/validation'\n",
    "    train_annotation_channel = prefix + '/train_annotation'\n",
    "    validation_annotation_channel = prefix + '/validation_annotation'\n",
    "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
    "    if 'tif' in f:\n",
    "        ranNUM = random.randint(0,10000000)\n",
    "        file = f\n",
    "        file = file.replace('._','')\n",
    "        if 'segproj/liorp_training_data/train/' in f:\n",
    "            jpgpath = '/tmp/'+'liorp_'+file.split('/')[-2]+'_'+'raw.jpg'\n",
    "            s3.meta.client.download_file('meadata', file, jpgpath.replace('jpg','tif'))\n",
    "            image = imread(jpgpath.replace('jpg','tif'))\n",
    "#             image = clahe.apply(image)\n",
    "            inverted_img =  cv2.resize(image, (1024,1024), interpolation = cv2.INTER_AREA)\n",
    "#             inverted_img,_ = rolling_ball_filter(image, ball_radius = 20, spacing = 1, top=False)\n",
    "\n",
    "            num = int(''.join(filter(str.isdigit, str(inverted_img.dtype)))) - 1\n",
    "            image = exposure.rescale_intensity(inverted_img, out_range=(0, 2**num - 1))\n",
    "            imsave(jpgpath,img_as_ubyte(image))\n",
    "            sess.upload_data(path=jpgpath, bucket=bucket, key_prefix=train_channel)\n",
    "        elif 'segproj/liorp_training_data/val/' in f:\n",
    "            jpgpath = '/tmp/'+'liorp_'+file.split('/')[-2]+'_'+'raw.jpg'\n",
    "            s3.meta.client.download_file('meadata', file, jpgpath.replace('jpg','tif'))\n",
    "            image = imread(jpgpath.replace('jpg','tif'))\n",
    "#             image = clahe.apply(image)\n",
    "            inverted_img =  cv2.resize(image, (1024,1024), interpolation = cv2.INTER_AREA)\n",
    "#             inverted_img,_ = rolling_ball_filter(image, ball_radius = 20, spacing = 1, top=False)\n",
    "            num = int(''.join(filter(str.isdigit, str(inverted_img.dtype)))) - 1\n",
    "            image = exposure.rescale_intensity(inverted_img, out_range=(0, 2**num - 1))\n",
    "            imsave(jpgpath,img_as_ubyte(image))\n",
    "            sess.upload_data(path=jpgpath, bucket=bucket, key_prefix=validation_channel)\n",
    "    elif 'instances_ids.png' in f:\n",
    "        ranNUM = random.randint(0,10000000)\n",
    "        file = f\n",
    "        file = file.replace('._','')\n",
    "        if 'segproj/liorp_training_data/train/' in f:\n",
    "            pngpath = '/tmp/'+'liorp_'+file.split('/')[-2]+'_'+'raw.png'\n",
    "            s3.meta.client.download_file('meadata', file, pngpath)\n",
    "            im1 = pngread(pngpath)\n",
    "            num = int(''.join(filter(str.isdigit, str(im1.dtype)))) - 1\n",
    "            image = exposure.rescale_intensity(im1, out_range=(0, 2**num - 1))\n",
    "            image = img_as_ubyte(image)\n",
    "            im = mark_boundaries(image, im1, color = [0,0,0], outline_color = [0,0,0], mode='outer', background_label=0)\n",
    "            im2 = img_as_int(im)\n",
    "            im3 = np.zeros([im2.shape[0],im2.shape[1]])\n",
    "            im3 = im2[:,:,0]+im2[:,:,1]+im2[:,:,2]\n",
    "            im3 = np.uint8((im3>0))\n",
    "            im3 =  cv2.resize(im3, (1024,1024), interpolation = cv2.INTER_AREA)                 \n",
    "            pngsave(pngpath,im3, check_contrast=False)\n",
    "            sess.upload_data(path=pngpath, bucket=bucket, key_prefix=train_annotation_channel)\n",
    "        elif 'segproj/liorp_training_data/val/' in f:\n",
    "            pngpath = '/tmp/'+'liorp_'+file.split('/')[-2]+'_'+'raw.png'\n",
    "            s3.meta.client.download_file('meadata', file, pngpath)\n",
    "            im1 = pngread(pngpath)\n",
    "            num = int(''.join(filter(str.isdigit, str(im1.dtype)))) - 1\n",
    "            image = exposure.rescale_intensity(im1, out_range=(0, 2**num - 1))\n",
    "            image = img_as_ubyte(image)\n",
    "            im = mark_boundaries(image, im1, color = [0,0,0], outline_color = [0,0,0], mode='outer', background_label=0)\n",
    "            im2 = img_as_int(im)\n",
    "            im3 = np.zeros([im2.shape[0],im2.shape[1]])\n",
    "            im3 = im2[:,:,0]+im2[:,:,1]+im2[:,:,2]\n",
    "            im3 = np.uint8((im3>0))\n",
    "            im3 =  cv2.resize(im3, (1024,1024), interpolation = cv2.INTER_AREA)                 \n",
    "            pngsave(pngpath,im3, check_contrast=False)\n",
    "            sess.upload_data(path=pngpath, bucket=bucket, key_prefix=validation_annotation_channel)\n",
    "\n",
    "keys = [obj.key for obj in s3meadata.objects.all()]\n",
    "for key in keys:\n",
    "    t = threading.Thread(target = proccessliorfiles, args=(key,)).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import boto3\n",
    "# import numpy as np\n",
    "# from skimage import util \n",
    "# from skimage.util import img_as_ubyte\n",
    "# from skimage import exposure\n",
    "# from skimage.io import imread as pngread\n",
    "# from skimage.io import imsave as pngsave\n",
    "# from tifffile import imsave,imread\n",
    "# import cv2\n",
    "# from rolling_ball_filter import rolling_ball_filter\n",
    "# from scipy import ndimage \n",
    "# from skimage import morphology\n",
    "# from skimage.morphology import watershed\n",
    "# from skimage.feature import peak_local_max\n",
    "# from skimage.segmentation import mark_boundaries\n",
    "# from skimage import color\n",
    "# from skimage import img_as_int\n",
    "# import matplotlib.pyplot as plt\n",
    "# # Copy tif from s3 to local sagemaker\n",
    "# files = []\n",
    "# s3 = boto3.resource('s3')\n",
    "# s3_resource = boto3.resource('s3')\n",
    "# s3meadata = s3_resource.Bucket(name='meadata')\n",
    "\n",
    "# keys = [obj.key for obj in s3meadata.objects.all() if 'segproj/usiigaci_train_data/train/' in obj.key and 'instances_ids.png' in obj.key]\n",
    "# file = keys[3]\n",
    "# s3.meta.client.download_file('meadata', file, '/tmp/instances_ids.png')\n",
    "# pngpath = '/tmp/'+'fig8_'+file.split('/')[-2]+'_'+'raw.png'\n",
    "# im1 = pngread('/tmp/instances_ids.png')\n",
    "# num = int(''.join(filter(str.isdigit, str(im1.dtype)))) - 1\n",
    "# image = exposure.rescale_intensity(im1, out_range=(0, 2**num - 1))\n",
    "# image = img_as_ubyte(image)\n",
    "# im = mark_boundaries(image, im1, color = [0,0,0], outline_color = [0,0,0], mode='outer', background_label=0)\n",
    "# im2 = img_as_int(im)\n",
    "# im3 = np.zeros([im2.shape[0],im2.shape[1]])\n",
    "# im3 = im2[:,:,0]+im2[:,:,1]+im2[:,:,2]\n",
    "# im3 = np.uint8((im3>0))\n",
    "# im3 =  cv2.resize(im3, (1024,1024), interpolation = cv2.INTER_AREA)            \n",
    "# pngsave('/home/ec2-user/SageMaker/instances_ids.png',im3, check_contrast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "label_map = { \"scale\": 1 }\n",
    "with open('train_label_map.json', 'w') as lm_fname:\n",
    "    json.dump(label_map, lm_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-eu-west-1-102554356212/fresh_train_trial/output\n"
     ]
    }
   ],
   "source": [
    "s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "print(s3_output_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the sagemaker estimator object.\n",
    "ss_model = sagemaker.estimator.Estimator(training_image,\n",
    "                                         role, \n",
    "                                         train_instance_count = 1, \n",
    "                                         train_instance_type = 'ml.p3.16xlarge',\n",
    "                                         train_volume_size = 150, # size in gb on s3 to reserve\n",
    "                                         train_max_run = 360000,\n",
    "                                         output_path = s3_output_location,\n",
    "                                         base_job_name = 'fresh-train-trial',\n",
    "                                         sagemaker_session = sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup hyperparameters \n",
    "ss_model.set_hyperparameters(backbone='resnet-101', # This is the encoder. Other option is resnet-50\n",
    "                             algorithm='deeplab', # This is the decoder. Other option is 'psp' and 'deeplab'                             \n",
    "                             use_pretrained_model='False', # Use the pre-trained model.\n",
    "                             crop_size=256, # Size of image random crop.                             \n",
    "                             num_classes=2, # Background + cell \n",
    "                             epochs=500, # Number of epochs to run.\n",
    "                             learning_rate=0.00058, momentum = 0.916, weight_decay = 0.000114,                           \n",
    "                             optimizer='adagrad', # Other options include 'adam', 'rmsprop', 'nag', 'adagrad'.\n",
    "                             lr_scheduler='poly', # Other options include 'cosine' and 'step'.                           \n",
    "                             mini_batch_size=41, # Setup some mini batch size.\n",
    "                             validation_mini_batch_size=16, #try larger batch sizes maybe? \n",
    "                             early_stopping=True, # Turn on early stopping. If OFF, other early stopping parameters are ignored.\n",
    "                             early_stopping_patience=20, # Tolerate these many epochs if the mIoU doens't increase.\n",
    "                             early_stopping_min_epochs=10, # No matter what, run these many number of epochs.                             \n",
    "                             num_training_samples=345) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create full bucket names\n",
    "s3_train_data = 's3://{}/{}'.format(bucket, train_channel)\n",
    "s3_validation_data = 's3://{}/{}'.format(bucket, validation_channel)\n",
    "s3_train_annotation = 's3://{}/{}'.format(bucket, train_annotation_channel)\n",
    "s3_validation_annotation = 's3://{}/{}'.format(bucket, validation_annotation_channel)\n",
    "\n",
    "distribution = 'FullyReplicated'\n",
    "# Create sagemaker s3_input objects\n",
    "train_data = sagemaker.session.s3_input(s3_train_data, distribution=distribution, \n",
    "                                        content_type='image/jpeg', s3_data_type='S3Prefix')\n",
    "validation_data = sagemaker.session.s3_input(s3_validation_data, distribution=distribution, \n",
    "                                        content_type='image/jpeg', s3_data_type='S3Prefix')\n",
    "train_annotation = sagemaker.session.s3_input(s3_train_annotation, distribution=distribution, \n",
    "                                        content_type='image/png', s3_data_type='S3Prefix')\n",
    "validation_annotation = sagemaker.session.s3_input(s3_validation_annotation, distribution=distribution, \n",
    "                                        content_type='image/png', s3_data_type='S3Prefix')\n",
    "\n",
    "data_channels = {'train': train_data, \n",
    "                 'validation': validation_data,\n",
    "                 'train_annotation': train_annotation, \n",
    "                 'validation_annotation':validation_annotation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "ss_model.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #HyperParam Optimization\n",
    "# !pip3 install smac\n",
    "\n",
    "# from smac.configspace import ConfigurationSpace\n",
    "# from ConfigSpace.hyperparameters import CategoricalHyperparameter, UniformFloatHyperparameter, UniformIntegerHyperparameter\n",
    "\n",
    "# cs = ConfigurationSpace()\n",
    "\n",
    "# backbone = CategoricalHyperparameter(\"backbone\", ['resnet-50','resnet-101'], default_value=\"resnet-50\")\n",
    "# algorithm = CategoricalHyperparameter(\"algorithm\", ['psp','deeplab'], default_value=\"deeplab\")\n",
    "# lr_scheduler = CategoricalHyperparameter(\"lr_scheduler\", ['poly','cosine','step'], default_value=\"poly\")\n",
    "# optimizer = CategoricalHyperparameter(\"optimizer\", ['resnet-50','resnet-101'], default_value=\"resnet-50\")\n",
    "# use_pretrained_model = CategoricalHyperparameter(\"use_pretrained_model\", ['False','True'], default_value=\"True\")\n",
    "\n",
    "# crop_size = UniformIntegerHyperparameter(\"crop_size\", 64,512, default_value=240)\n",
    "# early_stopping_min_epochs = UniformIntegerHyperparameter(\"early_stopping_min_epochs\", 10,60, default_value=10)\n",
    "# early_stopping_patience = UniformIntegerHyperparameter(\"early_stopping_patience\", 2,20, default_value=5)\n",
    "# epochs = UniformIntegerHyperparameter(\"epochs\", 10,1000, default_value=100)\n",
    "# mini_batch_size = UniformIntegerHyperparameter(\"mini_batch_size\", 8,56, default_value=16)\n",
    "# validation_mini_batch_size = UniformIntegerHyperparameter(\"validation_mini_batch_size\", 8,48, default_value=16)\n",
    "# early_stopping_patience = UniformIntegerHyperparameter(\"early_stopping_patience\", 10,50, default_value=20)\n",
    "# early_stopping_min_epochs = UniformIntegerHyperparameter(\"early_stopping_min_epochs\", 10,50, default_value=10)\n",
    "\n",
    "# learning_rate = UniformFloatHyperparameter(\"learning_rate\", 0.0001,0.1, default_value=0.0001)\n",
    "\n",
    "\n",
    "# cs.add_hyperparameter([backbone,algorithm,lr_scheduler,optimizer,use_pretrained_model,\n",
    "#                        crop_size, early_stopping_min_epochs, early_stopping_patience,\n",
    "#                        epochs, mini_batch_size, validation_mini_batch_size, early_stopping_patience, early_stopping_min_epochs])\n",
    "\n",
    "# def ssmodel_from_cfg(cfg):\n",
    "\n",
    "#     ss_model.fit(inputs=data_channels, logs=True)\n",
    "\n",
    "#     score = model_result\n",
    "#     return 1-score \n",
    "\n",
    "\n",
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "# paramdict = {'backbone' :   CategoricalParameter(['resnet-50','resnet-101']),\n",
    "#              'algorithm': CategoricalParameter(['psp','deeplab']),\n",
    "#             'crop_size' :   IntegerParameter(64,512),\n",
    "#             'early_stopping_min_epochs':    IntegerParameter(10,60),\n",
    "#             'early_stopping_patience':    IntegerParameter(2,20),\n",
    "#             'epochs' :   IntegerParameter(10,1000),\n",
    "#             'learning_rate':    ContinuousParameter(0.0001, 0.1),\n",
    "#             'lr_scheduler':    CategoricalParameter(['poly','cosine','step']),\n",
    "#             'optimizer' :   CategoricalParameter(['rmsprop','adam','nag', 'adagrad']),\n",
    "#             'use_pretrained_model'  :  CategoricalParameter([False,True]),\n",
    "#             'mini_batch_size': IntegerParameter(8,48), \n",
    "#             'validation_mini_batch_size':IntegerParameter(8,48),\n",
    "#             'early_stopping_patience': IntegerParameter(10,50), \n",
    "#             'early_stopping_min_epochs':IntegerParameter(10,50)}\n",
    "\n",
    "\n",
    "paramdict = {'weight_decay': ContinuousParameter(0.0001, 0.5), \n",
    "             'optimizer': CategoricalParameter(['rmsprop','adam','nag', 'adagrad']),\n",
    "             'learning_rate': ContinuousParameter(0.0001, 0.1), \n",
    "             'momentum': ContinuousParameter(0.5, 1), \n",
    "             'mini_batch_size': IntegerParameter(8,48) }\n",
    "\n",
    "objective_metric_name = 'Validation-mIOU'\n",
    "metric_definitions = [{'Name': 'validation-mIOU',\n",
    "                       'Regex': 'validation mIOU=([0-9\\\\.]+)'}]\n",
    "\n",
    "tuner = HyperparameterTuner(ss_model,\n",
    "                            objective_metric_name = 'validation:mIOU',\n",
    "                            hyperparameter_ranges = paramdict,\n",
    "                            max_jobs=100,\n",
    "                            max_parallel_jobs=2)\n",
    "\n",
    "tuner.fit(inputs=data_channels)\n",
    "# bayes_metrics = sagemaker.HyperparameterTuningJobAnalytics(tuner._current_job_name).dataframe()\n",
    "# bayes_metrics.sort_values(['FinalObjectiveValue'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_predictor = ss_model.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us convert the image to bytearray before we supply it to our endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import io \n",
    "clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(64,64))\n",
    "# images/liorp_181106_2_raw.jpg\n",
    "# filename = \"/home/ec2-user/SageMaker/itzik_images_cropped/_B37-543-2_12.tif\"\n",
    "filename = \"/home/ec2-user/SageMaker/images/190221_LV_ver2.tif\"\n",
    "im = imread(filename)\n",
    "# im = clahe.apply(im)\n",
    "# im,_ = rolling_ball_filter(im, ball_radius = 20, spacing = 1, top=False)\n",
    "im =  cv2.resize(im, (720,720), interpolation = cv2.INTER_AREA)\n",
    "num = int(''.join(filter(str.isdigit, str(im.dtype)))) - 1\n",
    "im = img_as_ubyte(exposure.rescale_intensity(im, out_range=(0, 2**num - 1)))\n",
    "pngsave(filename.replace('tif','jpg'), im)\n",
    "\n",
    "with open(filename.replace('tif','jpg'), 'rb') as image:\n",
    "    img = image.read()    \n",
    "    img = bytearray(img)\n",
    "\n",
    "fig1 = plt.figure(figsize=(10, 10)) # create a figure with the default size \n",
    "ax1 = fig1.add_subplot(1,1,1) \n",
    "ax1.imshow(Image.open(io.BytesIO(img)), interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "ss_predictor.content_type = 'image/jpeg'\n",
    "ss_predictor.accept = 'image/png'\n",
    "return_img = ss_predictor.predict(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us display the segmentation mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import io\n",
    "from scipy import ndimage as ndi\n",
    "from skimage.feature import peak_local_max\n",
    "from skimage.segmentation import watershed\n",
    "import seaborn as sns\n",
    "from skimage.color import label2rgb\n",
    "\n",
    "num_classes = 2\n",
    "mask = np.array(Image.open(io.BytesIO(return_img)))\n",
    "distance = ndi.distance_transform_edt(mask)\n",
    "local_maxi = peak_local_max(distance, labels=mask, footprint=np.ones((3, 3)), indices=False)\n",
    "markers = ndi.label(local_maxi)[0]\n",
    "labels = watershed(-distance, markers, mask=mask)\n",
    "pngsave('/home/ec2-user/SageMaker/testresult_mask.tif', mask)\n",
    "\n",
    "fig1 = plt.figure(figsize=(20, 20)) # create a figure with the default size \n",
    "ax1 = fig1.add_subplot(2,2,1) \n",
    "result = label2rgb(label = labels, image = exposure.rescale_intensity(im.astype(np.float), out_range=(-1, 1)))\n",
    "ax1.imshow(result)\n",
    "ax2 = fig1.add_subplot(2,2,2) \n",
    "ax2.imshow(labels)\n",
    "plt.show()\n",
    "np.unique(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the Endpoint\n",
    "Having an endpoint running will incur some costs. Therefore as a clean-up job, we should delete the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(ss_predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import boto3\n",
    "# import json\n",
    "# from sagemaker.amazon.record_pb2 import Record\n",
    "# import mxnet as mx\n",
    "# from PIL import Image\n",
    "# import numpy as np\n",
    "# import io\n",
    "\n",
    "# endpoint = \"fresh-train-trial-2019-07-16-12-00-41-881\"\n",
    " \n",
    "# runtime = boto3.Session().client('sagemaker-runtime')\n",
    " \n",
    "# # Read image into memory\n",
    "# # Send image via InvokeEndpoint API\n",
    "# response = runtime.invoke_endpoint(EndpointName=endpoint, ContentType='image/jpeg', Accept = 'image/png', Body=img)\n",
    "# a = response['Body'].read()\n",
    "# mask = np.array(Image.open(io.BytesIO(a)))\n",
    "# plt.imshow(mask)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sagemaker.Session().delete_endpoint(endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
