{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-15T13:52:51.949187Z",
     "start_time": "2018-12-15T13:52:29.593597Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Segmentation Functions\n",
    "import sys\n",
    "sys.path.insert(0, '/home/mestalbet/Segment2P/')\n",
    "from segment_functions import *\n",
    "#--------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/mestalbet/bucket/PythonScripts/Results_LiorImages/inference/pngs/\"\n",
    "model_path_1 = \"/home/mestalbet/Segment2P/TrainWeights/mask_rcnn_cell_0100.h5\"\n",
    "model_path_2 = \"/home/mestalbet/Segment2P/TrainWeights/mask_rcnn_cell_0198.h5\"\n",
    "model_path_3 = \"/home/mestalbet/Segment2P/TrainWeights/mask_rcnn_cell_0199.h5\"\n",
    "model_path_4 = \"/home/mestalbet/Segment2P/TrainWeights/mask_rcnn_cell_0200.h5\"\n",
    "model_list = [model_path_1, model_path_2, model_path_3, model_path_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Configuration class\n",
    "chunksize = 1\n",
    "class CellInferenceConfig(cellConfig):\n",
    "        # Set batch size to 1 to run one image at a time\n",
    "        GPU_COUNT = 1\n",
    "        IMAGES_PER_GPU = chunksize\n",
    "        # Don't resize imager for inferencing\n",
    "        IMAGE_RESIZE_MODE = \"pad64\"\n",
    "        # Non-max suppression threshold to filter RPN proposals.\n",
    "        # You can increase this during training to generate more propsals.\n",
    "        RPN_NMS_THRESHOLD = 0.7\n",
    "        # define the folder path to data for prediction\n",
    "        global data_dir\n",
    "        all_files = []\n",
    "        sub_directory = []\n",
    "        for root, dirs, files in os.walk(data_dir):\n",
    "            for file in files:\n",
    "                relativePath = os.path.relpath(root, data_dir)\n",
    "                if relativePath == \".\":\n",
    "                    relativePath = \"\"\n",
    "                all_files.append(\n",
    "                    (relativePath.count(os.path.sep), relativePath, file))\n",
    "        all_files.sort(reverse=True)\n",
    "        for (count, folder), files in groupby(all_files, itemgetter(0, 1)):\n",
    "            sub_directory.append(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# imgpath = \"/home/mestalbet/PythonScripts/Results_LiorImages\"\n",
    "# alignedpath = [os.path.join(imgpath,f) for f in os.listdir(imgpath) if f.endswith('tif')]\n",
    "# [cleanliorimage(a) for a in alignedpath]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mestalbet/anaconda3/envs/segproj/lib/python3.6/site-packages/tensorflow/python/ops/sparse_ops.py:1165: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n"
     ]
    }
   ],
   "source": [
    "imgpath = \"/home/mestalbet/PythonScripts/Results_LiorImages\"\n",
    "output_json = \"gs://segproj/PythonScripts/submit_data/inputs.json\"\n",
    "imgpath = os.path.join(imgpath,'inference')\n",
    "# os.system(\"rm %s\" % (output_json))\n",
    "createjson(imgpath, output_json, model_path_1, data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit job to cloud\n",
    "# ------------------------------------------#\n",
    "# Define batch job submission vfunction\n",
    "def make_batch_job_body(project_name, input_paths, output_path,\n",
    "                        model_name, region, data_format='JSON',\n",
    "                        version_name=None, max_worker_count=None,\n",
    "                        runtime_version=None):\n",
    "\n",
    "    project_id = 'projects/{}'.format(project_name)\n",
    "    model_id = '{}/models/{}'.format(project_id, model_name)\n",
    "    if version_name:\n",
    "        version_id = '{}/versions/{}'.format(model_id, version_name)\n",
    "\n",
    "    # Make a jobName of the format \"model_name_batch_predict_YYYYMMDD_HHMMSS\"\n",
    "    timestamp = time.strftime('%Y%m%d_%H%M%S', time.gmtime())\n",
    "\n",
    "    # Make sure the project name is formatted correctly to work as the basis\n",
    "    # of a valid job name.\n",
    "    clean_project_name = re.sub(r'\\W+', '_', project_name)\n",
    "\n",
    "    job_id = '{}_{}_{}'.format(clean_project_name, model_name,\n",
    "                           timestamp)\n",
    "\n",
    "    # Start building the request dictionary with required information.\n",
    "    body = {'jobId': job_id,\n",
    "            'predictionInput': {\n",
    "                'dataFormat': data_format,\n",
    "                'inputPaths': input_paths,\n",
    "                'outputPath': output_path,\n",
    "                'region': region}}\n",
    "\n",
    "    # Use the version if present, the model (its default version) if not.\n",
    "    if version_name:\n",
    "        body['predictionInput']['versionName'] = version_id\n",
    "    else:\n",
    "        body['predictionInput']['modelName'] = model_id\n",
    "\n",
    "    # Only include a maximum number of workers or a runtime version if specified.\n",
    "    # Otherwise let the service use its defaults.\n",
    "    if max_worker_count:\n",
    "        body['predictionInput']['maxWorkerCount'] = max_worker_count\n",
    "\n",
    "    if runtime_version:\n",
    "        body['predictionInput']['runtimeVersion'] = runtime_version\n",
    "\n",
    "    return body\n",
    "\n",
    "# Project definitions\n",
    "project_name = \"divine-builder-142611\"\n",
    "credentials_path = \"/home/mestalbet/bucket/PythonScripts/Segmentation Project-a5a157bd9401.json\"\n",
    "project_id = 'projects/{}'.format(project_name)\n",
    "input_paths = \"gs://segproj/PythonScripts/submit_data/\"\n",
    "output_path = \"gs://segproj/PythonScripts/cloud_output/\"\n",
    "model_name = \"segmentation\"\n",
    "region = \"us-central1\"\n",
    "\n",
    "# Submit job\n",
    "version_names = ['Model0100_Ver1_0','Model0198_Ver1_0','Model0199_Ver1_0','Model0200_Ver1_0']\n",
    "for vn in version_names:\n",
    "    op = os.path.join(output_path,vn)\n",
    "    if not os.path.exists(op):\n",
    "        os.makedirs(op)\n",
    "\n",
    "    dirs = os.listdir(op) \n",
    "    vernum = [int(d[-1]) for d in dirs]\n",
    "    if not vernum:\n",
    "        vernum = 0\n",
    "    else:\n",
    "        vernum = np.max(vernum)\n",
    "    op = os.path.join(op, \"run_%d\" % (vernum+1))\n",
    "    if not os.path.exists(op):\n",
    "        os.makedirs(op)\n",
    "        \n",
    "    batch_predict_body = make_batch_job_body(project_name, input_paths, op,\n",
    "                                             model_name, region, data_format='JSON',\n",
    "                                             version_name=vn, max_worker_count=None,\n",
    "                                             runtime_version=None)\n",
    "    ml = discovery.build('ml', 'v1')\n",
    "    request = ml.projects().jobs().create(parent=project_id,body=batch_predict_body)\n",
    "    response = request.execute()\n",
    "\n",
    "# developerKey=\"AIzaSyCmugrkm9rIUpn8AnAKxX8KaKZJU5Qjz6Q\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelpath = model_list[0]\n",
    "# K.clear_session()\n",
    "# model = modellib.MaskRCNN(mode=\"inference\", config=CellInferenceConfig(), model_dir=data_dir)\n",
    "\n",
    "# respath = \"/home/mestalbet/bucket/PythonScripts/cloud_output/\"\n",
    "# resfiles = glob(respath+'prediction.results*')\n",
    "# resfiles = [r for r in resfiles if getsize(r)>0]\n",
    "# ishape = (1024, 1024, 3)\n",
    "# mshape = (2048, 2048, 3)\n",
    "# window = np.array([   0,    0, 2048, 2048])\n",
    "# final_masks=[]\n",
    "# for resfile in resfiles:\n",
    "#     json_data=open(resfile).read()\n",
    "#     data = json.loads(json_data)\n",
    "#     _, _, _, fm = model.unmold_detections(\n",
    "#                                 np.asarray(data['mrcnn_detection/Reshape_1:0']), \n",
    "#                                 np.asarray(data['mrcnn_mask/Reshape_1:0']),\n",
    "#                                 ishape, mshape, window)\n",
    "#     if (fm.shape[0]!=1024 or fm.shape[2]<1):\n",
    "#         print(\"Image Output Size Error\")\n",
    "#     else: \n",
    "#         final_masks.append(np.argmax(fm,2))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "respath = \"/home/mestalbet/bucket/PythonScripts/cloud_output/\"\n",
    "modelpath = model_list[0]\n",
    "K.clear_session()\n",
    "model = modellib.MaskRCNN(mode=\"inference\", config=CellInferenceConfig(), model_dir=data_dir)\n",
    "\n",
    "model_cp=[]\n",
    "model_ver=[]\n",
    "image_num=[] \n",
    "run_num=[]\n",
    "mask=[]\n",
    "ishape = (1024, 1024, 3)\n",
    "mshape = (2048, 2048, 3)\n",
    "window = np.array([   0,    0, 2048, 2048])\n",
    "for (dirpath, dirnames, filenames) in os.walk(respath):\n",
    "    for file in filenames:\n",
    "        resfile = os.path.join(dirpath, file)\n",
    "        if ('prediction.results' in resfile and getsize(resfile)>0):\n",
    "            json_data=open(resfile).read()\n",
    "            data = json.loads(json_data)\n",
    "            _, _, _, fm = model.unmold_detections(\n",
    "                                        np.asarray(data['mrcnn_detection/Reshape_1:0']), \n",
    "                                        np.asarray(data['mrcnn_mask/Reshape_1:0']),\n",
    "                                        ishape, mshape, window)\n",
    "            if (fm.shape[0]!=1024 or fm.shape[2]<1):\n",
    "                fm = np.zeros((1024,1024,2))\n",
    "            else:\n",
    "                model_cp.append(dirpath.split('/')[-2].split('_')[0])\n",
    "                model_ver.append(dirpath.split('/')[-2].split('_')[1])\n",
    "                image_num.append(int(file.split('-')[1]))\n",
    "                run_num.append(int(dirpath.split('/')[-1].split('_')[-1]))\n",
    "                mask.append(np.argmax(fm,2))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'model_cp':model_cp,\n",
    "                   'model_ver':model_ver,\n",
    "                   'image_num':image_num, \n",
    "                   'run_num':run_num,\n",
    "                   'mask':mask})\n",
    "df.sort_values(['image_num','model_cp','run_num'],inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df[0:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {'image_num':[], 'mask_avg':[]}\n",
    "for imnum in np.unique(df['image_num'].values):\n",
    "    masks = df[df['image_num']==imnum]['mask'].values.tolist()\n",
    "    results['image_num'].append(imnum)\n",
    "    results['mask_avg'].append(merge_multiple_detections(masks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results  = pd.DataFrame(results)\n",
    "plt.imshow(results['mask_avg'].iloc[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(mask1, mask2):\n",
    "        \"\"\"\n",
    "        Computes Intersection over Union score for two binary masks.\n",
    "        :param mask1: numpy array\n",
    "        :param mask2: numpy array\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        intersection = np.sum((mask1 + mask2) > 1)\n",
    "        union = np.sum((mask1 + mask2) > 0)\n",
    "\n",
    "        return intersection / float(union)\n",
    "\n",
    "def compute_overlap(mask1, mask2):\n",
    "    intersection = np.sum((mask1 + mask2) > 1)\n",
    "\n",
    "    overlap1 = intersection / float(np.sum(mask1))\n",
    "    overlap2 = intersection / float(np.sum(mask2))\n",
    "    return overlap1, overlap2\n",
    "\n",
    "def sort_mask_by_cells(mask, min_size=50):\n",
    "    \"\"\"\n",
    "    Returns size of each cell.\n",
    "    :param mask:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    cell_num = np.unique(mask)\n",
    "    cell_sizes = [(cell_id, len(np.where(mask == cell_id)[0]))\n",
    "                  for cell_id in cell_num if cell_id != 0]\n",
    "\n",
    "    cell_sizes = [x for x in sorted(\n",
    "        cell_sizes, key=lambda x: x[1], reverse=True) if x[1 > min_size]]\n",
    "\n",
    "    return cell_sizes\n",
    "    \n",
    "def merge_multiple_detections(masks):\n",
    "        \"\"\"\n",
    "\n",
    "        :param masks:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        cell_counter = 0\n",
    "        final_mask = np.zeros(masks[0].shape)\n",
    "\n",
    "        masks_stats = [sort_mask_by_cells(mask) for mask in masks]\n",
    "        cells_left = sum([len(stats) for stats in masks_stats])\n",
    "\n",
    "        while cells_left > 0:\n",
    "            # Choose the biggest cell from available\n",
    "            cells = [stats[0][1] if len(\n",
    "                stats) > 0 else 0 for stats in masks_stats]\n",
    "            reference_mask = cells.index(max(cells))\n",
    "\n",
    "            reference_cell = masks_stats[reference_mask].pop(0)[0]\n",
    "\n",
    "            # Prepare binary mask for cell chosen for comparison\n",
    "            cell_location = np.where(masks[reference_mask] == reference_cell)\n",
    "\n",
    "            cell_mask = np.zeros(final_mask.shape)\n",
    "            cell_mask[cell_location] = 1\n",
    "\n",
    "            masks[reference_mask][cell_location] = 0\n",
    "\n",
    "            # Mask for storing temporary results\n",
    "            tmp_mask = np.zeros(final_mask.shape)\n",
    "            tmp_mask += cell_mask\n",
    "\n",
    "            for mask_id, mask in enumerate(masks):\n",
    "                # For each mask left\n",
    "                if mask_id != reference_mask:\n",
    "                    # # Find overlapping cells on other masks\n",
    "                    overlapping_cells = list(np.unique(mask[cell_location]))\n",
    "\n",
    "                    try:\n",
    "                        overlapping_cells.remove(0)\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "\n",
    "                    # # If only one overlapping, check IoU and update tmp mask if high\n",
    "                    if len(overlapping_cells) == 1:\n",
    "                        overlapping_cell_mask = np.zeros(final_mask.shape)\n",
    "                        overlapping_cell_mask[np.where(\n",
    "                            mask == overlapping_cells[0])] = 1\n",
    "\n",
    "                        iou = compute_iou(cell_mask, overlapping_cell_mask)\n",
    "                        if iou >= IOU_THRESHOLD:\n",
    "                            # Add cell to temporary results and remove from stats and mask\n",
    "                            tmp_mask += overlapping_cell_mask\n",
    "                            idx = [i for i, cell in enumerate(\n",
    "                                masks_stats[mask_id]) if cell[0] == overlapping_cells[0]][0]\n",
    "                            masks_stats[mask_id].pop(idx)\n",
    "                            mask[np.where(mask == overlapping_cells[0])] = 0\n",
    "\n",
    "                    # # If more than one overlapping check area overlapping\n",
    "                    elif len(overlapping_cells) > 1:\n",
    "                        overlapping_cell_masks = [\n",
    "                            np.zeros(final_mask.shape) for _ in overlapping_cells]\n",
    "\n",
    "                        for i, cell_id in enumerate(overlapping_cells):\n",
    "                            overlapping_cell_masks[i][np.where(\n",
    "                                mask == cell_id)] = 1\n",
    "\n",
    "                        for cell_id, overlap_mask in zip(overlapping_cells, overlapping_cell_masks):\n",
    "                            overlap_score, _ = compute_overlap(\n",
    "                                overlap_mask, cell_mask)\n",
    "\n",
    "                            if overlap_score >= OVERLAP_THRESHOLD:\n",
    "                                tmp_mask += overlap_mask\n",
    "\n",
    "                                mask[np.where(mask == cell_id)] = 0\n",
    "                                idx = [i for i, cell in enumerate(masks_stats[mask_id])\n",
    "                                       if cell[0] == cell_id][0]\n",
    "                                masks_stats[mask_id].pop(idx)\n",
    "\n",
    "                    # # If none overlapping do nothing\n",
    "\n",
    "            if len(np.unique(tmp_mask)) > 1:\n",
    "                cell_counter += 1\n",
    "                final_mask[np.where(tmp_mask >= MIN_DETECTIONS)] = cell_counter\n",
    "\n",
    "            cells_left = sum([len(stats) for stats in masks_stats])\n",
    "\n",
    "        bin_mask = np.zeros(final_mask.shape)\n",
    "        bin_mask[np.where(final_mask > 0)] = 255\n",
    "        return(final_mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notify_time": "30",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "328px",
    "width": "719px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
