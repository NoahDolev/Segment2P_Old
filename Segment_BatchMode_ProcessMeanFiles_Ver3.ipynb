{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-15T13:52:51.949187Z",
     "start_time": "2018-12-15T13:52:29.593597Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# General System stuff\n",
    "import sys\n",
    "import gc\n",
    "import glob\n",
    "import os\n",
    "from os.path import getsize\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "IOU_THRESHOLD = 0.6\n",
    "OVERLAP_THRESHOLD = 0.8\n",
    "MIN_DETECTIONS = 1\n",
    "\n",
    "#--------------------------------------#\n",
    "\n",
    "# Processing stuff\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import operator\n",
    "from operator import itemgetter\n",
    "from itertools import groupby\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "#--------------------------------------#\n",
    "\n",
    "# Deep learning and GPU stuff\n",
    "from keras import backend as K\n",
    "from numba import cuda\n",
    "import tensorflow as tf \n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, QuantileTransformer\n",
    "from tensorflow.python.framework import graph_util\n",
    "from tensorflow.python.saved_model import signature_constants\n",
    "from tensorflow.python.saved_model import tag_constants\n",
    "import googleapiclient.discovery as discovery\n",
    "\n",
    "sys.path.insert(0, '/home/mestalbet/Segment2P/mrcnn')\n",
    "from mrcnn import model as modellib\n",
    "from mrcnn import utils\n",
    "from train import cellConfig\n",
    "\n",
    "#--------------------------------------#\n",
    "\n",
    "# Image Stuff\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from tifffile import imsave,imread\n",
    "from skimage import exposure\n",
    "from skimage.color import rgb2gray\n",
    "from skimage import io as skio\n",
    "from skimage import util\n",
    "\n",
    "#--------------------------------------#\n",
    "\n",
    "# File Handling\n",
    "import datetime\n",
    "import time\n",
    "from glob import glob\n",
    "import csv\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import base64\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "\n",
    "#--------------------------------------#\n",
    "\n",
    "# GUI\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "#--------------------------------------#\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import colorsys\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "import plotly.plotly as py\n",
    "import plotly.offline as offline\n",
    "plotly.offline.init_notebook_mode(connected=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outpath = \"/home/mestalbet/PythonScripts/Results_LiorImages/\"\n",
    "data_dir = \"gs://segproj/PythonScripts/Results_LiorImages/inference/pngs/\"\n",
    "# data_dir = os.path.join(outpath,\"inference/\")\n",
    "model_path_1 = \"/home/mestalbet/Segment2P/TrainWeights/mask_rcnn_cell_0100.h5\"\n",
    "model_path_2 = \"/home/mestalbet/Segment2P/TrainWeights/mask_rcnn_cell_0198.h5\"\n",
    "model_path_3 = \"/home/mestalbet/Segment2P/TrainWeights/mask_rcnn_cell_0199.h5\"\n",
    "model_path_4 = \"/home/mestalbet/Segment2P/TrainWeights/mask_rcnn_cell_0200.h5\"\n",
    "model_list = [model_path_1, model_path_2, model_path_3, model_path_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Configuration class\n",
    "chunksize = 1\n",
    "class CellInferenceConfig(cellConfig):\n",
    "        # Set batch size to 1 to run one image at a time\n",
    "        GPU_COUNT = 1\n",
    "        IMAGES_PER_GPU = chunksize\n",
    "        # Don't resize imager for inferencing\n",
    "        IMAGE_RESIZE_MODE = \"pad64\"\n",
    "        # Non-max suppression threshold to filter RPN proposals.\n",
    "        # You can increase this during training to generate more propsals.\n",
    "        RPN_NMS_THRESHOLD = 0.7\n",
    "        # define the folder path to data for prediction\n",
    "        global data_dir\n",
    "#         all_files = []\n",
    "#         sub_directory = []\n",
    "#         for root, dirs, files in os.walk(data_dir):\n",
    "#             for file in files:\n",
    "#                 relativePath = os.path.relpath(root, data_dir)\n",
    "#                 if relativePath == \".\":\n",
    "#                     relativePath = \"\"\n",
    "#                 all_files.append(\n",
    "#                     (relativePath.count(os.path.sep), relativePath, file))\n",
    "#         all_files.sort(reverse=True)\n",
    "#         for (count, folder), files in groupby(all_files, itemgetter(0, 1)):\n",
    "#             sub_directory.append(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Setup Path Iterator ######\n",
    "\n",
    "#Delete old models\n",
    "# os.system(\"rm /home/mestalbet/bucket/PythonScripts/savedmodel/ver* -r\") \n",
    "\n",
    "dirs = os.listdir(\"/home/mestalbet/bucket/PythonScripts/savedmodel/\") \n",
    "vernum = [int(d[-1]) for d in dirs]\n",
    "if not vernum:\n",
    "    vernum = 0\n",
    "else:\n",
    "    vernum = np.max(vernum)\n",
    "model_dirpath = \"/home/mestalbet/bucket/PythonScripts/savedmodel/ver%d\" % (vernum+1)\n",
    "if not os.path.exists(model_dirpath):\n",
    "    os.makedirs(model_dirpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Save Each Model (different checkpoints)#####\n",
    "for modelpath in model_list: \n",
    "    # Load model\n",
    "    K.clear_session()\n",
    "    model = modellib.MaskRCNN(\n",
    "        mode=\"inference\", config=CellInferenceConfig(), model_dir=data_dir)\n",
    "    # Load weights from H5\n",
    "    model.load_weights(modelpath, by_name=True)\n",
    "    sess = K.get_session()\n",
    "\n",
    "    outputs = [output.name for output in model.keras_model.outputs]\n",
    "    outs = {str(o):sess.graph.get_tensor_by_name(o) for o in outputs}\n",
    "    output_names_all = [output.split(':')[0] for output in outputs]\n",
    "    \n",
    "    # Save model\n",
    "    tf.saved_model.simple_save(sess,\n",
    "                               model_dirpath+\"/savedmodel_%s/\" % modelpath.split('/')[-1][-7:-3],\n",
    "                               inputs={'input_image':model.keras_model.inputs[0], \n",
    "                                       'input_image_meta':model.keras_model.inputs[1],\n",
    "                                       'input_anchors':model.keras_model.inputs[2]},\n",
    "                               outputs=outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelpath = model_list[0]\n",
    "# K.clear_session()\n",
    "# model = modellib.MaskRCNN(mode=\"inference\", config=CellInferenceConfig(), model_dir=data_dir)\n",
    "# # Load weights from H5\n",
    "# model.load_weights(modelpath, by_name=True)\n",
    "# model = model.keras_model\n",
    "# sess = K.get_session()\n",
    "\n",
    "# outputs = [output.name for output in model.outputs]\n",
    "# outs = {str(o):sess.graph.get_tensor_by_name(o) for o in outputs}\n",
    "\n",
    "# outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "imgpath = \"/home/mestalbet/PythonScripts/Results_LiorImages/inference/\"\n",
    "predict_instance_json = \"/home/mestalbet/bucket/PythonScripts/submit_data/inputs.json\"\n",
    "# bucketpath = \"gs://segproj/PythonScripts/Results_LiorImages/inference/\"\n",
    "os.system(\"rm %s\" % (predict_instance_json))\n",
    "config=CellInferenceConfig()\n",
    "model = modellib.MaskRCNN(mode=\"inference\", config=config, model_dir=data_dir)\n",
    "model.load_weights(model_path_1, by_name=True)\n",
    "\n",
    "class ImageDataset(utils.Dataset):\n",
    "    def load_images(self, dataset_dir):\n",
    "        \"\"\"\n",
    "        Loads dataset images.\n",
    "        :param dataset_dir: string, path to dataset directory.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.add_class(\"cell\", 1, \"cell\")\n",
    "\n",
    "        image_ids = [fn for fn in os.listdir(dataset_dir)\n",
    "                     if any(fn.endswith(ext) for ext in ['tif', \"png\"])]\n",
    "\n",
    "        for image_id in image_ids:\n",
    "            self.add_image(\n",
    "                'cell',\n",
    "                image_id=os.path.splitext(image_id)[0],\n",
    "                path=os.path.join(dataset_dir, image_id)\n",
    "            )\n",
    "dataset = ImageDataset()\n",
    "dataset.load_images(imgpath)\n",
    "dataset.prepare()\n",
    "with open(predict_instance_json, \"w+\") as fp:\n",
    "    for image_id in dataset.image_ids[:300]:\n",
    "        image = dataset.load_image(image_id)\n",
    "        active_class_ids = np.zeros([dataset.num_classes], dtype=np.int32)\n",
    "        source_class_ids = dataset.source_class_ids[dataset.image_info[image_id][\"source\"]]\n",
    "        active_class_ids[source_class_ids] = 1\n",
    "        img, window, scale, padding, crop = utils.resize_image(\n",
    "                image,\n",
    "                min_dim=config.IMAGE_MIN_DIM,\n",
    "                min_scale=config.IMAGE_MIN_SCALE,\n",
    "                max_dim=config.IMAGE_MAX_DIM,\n",
    "                mode=config.IMAGE_RESIZE_MODE)\n",
    "        meta = np.array(\n",
    "            [image_id] +                  # size=1\n",
    "            list(image.shape) +           # size=3\n",
    "            list(img.shape) +             # size=3\n",
    "            list(window) +                # size=4 (y1, x1, y2, x2) in image cooredinates\n",
    "            [scale] +                     # size=1\n",
    "            list(active_class_ids)        # size=num_classes\n",
    "            )\n",
    "        anchors = model.get_anchors(image.shape)\n",
    "        json_data = {'input_image':image.tolist(),\n",
    "                     'input_image_meta':meta.tolist(),\n",
    "                     'input_anchors':anchors.tolist()} #,'key':int(image_id)\n",
    "        jline = json.dumps(json_data) + \"\\n\"\n",
    "        fp.write(jline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit job to cloud\n",
    "# ------------------------------------------#\n",
    "# Define batch job submission vfunction\n",
    "def make_batch_job_body(project_name, input_paths, output_path,\n",
    "                        model_name, region, data_format='JSON',\n",
    "                        version_name=None, max_worker_count=None,\n",
    "                        runtime_version=None):\n",
    "\n",
    "    project_id = 'projects/{}'.format(project_name)\n",
    "    model_id = '{}/models/{}'.format(project_id, model_name)\n",
    "    if version_name:\n",
    "        version_id = '{}/versions/{}'.format(model_id, version_name)\n",
    "\n",
    "    # Make a jobName of the format \"model_name_batch_predict_YYYYMMDD_HHMMSS\"\n",
    "    timestamp = time.strftime('%Y%m%d_%H%M%S', time.gmtime())\n",
    "\n",
    "    # Make sure the project name is formatted correctly to work as the basis\n",
    "    # of a valid job name.\n",
    "    clean_project_name = re.sub(r'\\W+', '_', project_name)\n",
    "\n",
    "    job_id = '{}_{}_{}'.format(clean_project_name, model_name,\n",
    "                           timestamp)\n",
    "\n",
    "    # Start building the request dictionary with required information.\n",
    "    body = {'jobId': job_id,\n",
    "            'predictionInput': {\n",
    "                'dataFormat': data_format,\n",
    "                'inputPaths': input_paths,\n",
    "                'outputPath': output_path,\n",
    "                'region': region}}\n",
    "\n",
    "    # Use the version if present, the model (its default version) if not.\n",
    "    if version_name:\n",
    "        body['predictionInput']['versionName'] = version_id\n",
    "    else:\n",
    "        body['predictionInput']['modelName'] = model_id\n",
    "\n",
    "    # Only include a maximum number of workers or a runtime version if specified.\n",
    "    # Otherwise let the service use its defaults.\n",
    "    if max_worker_count:\n",
    "        body['predictionInput']['maxWorkerCount'] = max_worker_count\n",
    "\n",
    "    if runtime_version:\n",
    "        body['predictionInput']['runtimeVersion'] = runtime_version\n",
    "\n",
    "    return body\n",
    "\n",
    "# Project definitions\n",
    "project_name = \"divine-builder-142611\"\n",
    "credentials_path = \"/home/mestalbet/bucket/PythonScripts/Segmentation Project-a5a157bd9401.json\"\n",
    "project_id = 'projects/{}'.format(project_name)\n",
    "input_paths = \"gs://segproj/PythonScripts/submit_data/\"\n",
    "output_path = \"gs://segproj/PythonScripts/cloud_output/\"\n",
    "model_name = \"segmentation\"\n",
    "region = \"us-central1\"\n",
    "\n",
    "# Submit job\n",
    "version_names = ['Model0100_Ver1_0','Model0198_Ver1_0','Model0199_Ver1_0','Model0200_Ver1_0']\n",
    "for vn in version_names:\n",
    "    op = os.path.join(output_path,vn)\n",
    "    if not os.path.exists(op):\n",
    "        os.makedirs(op)\n",
    "\n",
    "    dirs = os.listdir(op) \n",
    "    vernum = [int(d[-1]) for d in dirs]\n",
    "    if not vernum:\n",
    "        vernum = 0\n",
    "    else:\n",
    "        vernum = np.max(vernum)\n",
    "    op = os.path.join(op, \"run_%d\" % (vernum+1))\n",
    "    if not os.path.exists(op):\n",
    "        os.makedirs(op)\n",
    "        \n",
    "    batch_predict_body = make_batch_job_body(project_name, input_paths, op,\n",
    "                                             model_name, region, data_format='JSON',\n",
    "                                             version_name=vn, max_worker_count=None,\n",
    "                                             runtime_version=None)\n",
    "    ml = discovery.build('ml', 'v1')\n",
    "    request = ml.projects().jobs().create(parent=project_id,body=batch_predict_body)\n",
    "    response = request.execute()\n",
    "\n",
    "# developerKey=\"AIzaSyCmugrkm9rIUpn8AnAKxX8KaKZJU5Qjz6Q\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpath = model_list[0]\n",
    "K.clear_session()\n",
    "model = modellib.MaskRCNN(mode=\"inference\", config=CellInferenceConfig(), model_dir=data_dir)\n",
    "\n",
    "respath = \"/home/mestalbet/bucket/PythonScripts/cloud_output/\"\n",
    "resfiles = glob(respath+'prediction.results*')\n",
    "resfiles = [r for r in resfiles if getsize(r)>0]\n",
    "ishape = (1024, 1024, 3)\n",
    "mshape = (2048, 2048, 3)\n",
    "window = np.array([   0,    0, 2048, 2048])\n",
    "final_masks=[]\n",
    "for resfile in resfiles:\n",
    "    json_data=open(resfile).read()\n",
    "    data = json.loads(json_data)\n",
    "    _, _, _, fm = model.unmold_detections(\n",
    "                                np.asarray(data['mrcnn_detection/Reshape_1:0']), \n",
    "                                np.asarray(data['mrcnn_mask/Reshape_1:0']),\n",
    "                                ishape, mshape, window)\n",
    "    if (fm.shape[0]!=1024 or fm.shape[2]<1):\n",
    "        print(\"Image Output Size Error\")\n",
    "    else: \n",
    "        final_masks.append(np.argmax(fm,2))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "respath = \"/home/mestalbet/bucket/PythonScripts/cloud_output/\"\n",
    "modelpath = model_list[0]\n",
    "K.clear_session()\n",
    "model = modellib.MaskRCNN(mode=\"inference\", config=CellInferenceConfig(), model_dir=data_dir)\n",
    "\n",
    "model_cp=[]\n",
    "model_ver=[]\n",
    "image_num=[] \n",
    "run_num=[]\n",
    "mask=[]\n",
    "ishape = (1024, 1024, 3)\n",
    "mshape = (2048, 2048, 3)\n",
    "window = np.array([   0,    0, 2048, 2048])\n",
    "for (dirpath, dirnames, filenames) in os.walk(respath):\n",
    "    for file in filenames:\n",
    "        resfile = os.path.join(dirpath, file)\n",
    "        if ('prediction.results' in resfile and getsize(resfile)>0):\n",
    "            json_data=open(resfile).read()\n",
    "            data = json.loads(json_data)\n",
    "            _, _, _, fm = model.unmold_detections(\n",
    "                                        np.asarray(data['mrcnn_detection/Reshape_1:0']), \n",
    "                                        np.asarray(data['mrcnn_mask/Reshape_1:0']),\n",
    "                                        ishape, mshape, window)\n",
    "            if (fm.shape[0]!=1024 or fm.shape[2]<1):\n",
    "                print(\"Image Output Size Error: %s\" % (resfile))\n",
    "            else:\n",
    "                model_cp.append(dirpath.split('/')[-2].split('_')[0])\n",
    "                model_ver.append(dirpath.split('/')[-2].split('_')[1])\n",
    "                image_num.append(int(file.split('-')[1]))\n",
    "                run_num.append(int(dirpath.split('/')[-1].split('_')[-1]))\n",
    "                mask.append(np.argmax(fm,2))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'model_cp':model_cp,\n",
    "                   'model_ver':model_ver,\n",
    "                   'image_num':image_num, \n",
    "                   'run_num':run_num,\n",
    "                   'mask':final_masks})\n",
    "df[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = df.groupby(['model_cp','model_ver','image_num','run_num']).agg([np.sum])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (segproj)",
   "language": "python",
   "name": "segproj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "notify_time": "30",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "328px",
    "width": "719px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
