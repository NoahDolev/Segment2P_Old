{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necesssary packages <a class=\"anchor\" id=\"chap1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-15T13:52:51.949187Z",
     "start_time": "2018-12-15T13:52:29.593597Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import gc\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, QuantileTransformer\n",
    "from skimage import util\n",
    "from numba import cuda\n",
    "import tensorflow as tf \n",
    "from caiman.motion_correction import MotionCorrect, tile_and_correct, motion_correction_piecewise\n",
    "import caiman as cm\n",
    "\n",
    "sys.path.insert(0, '/home/mestalbet/PythonScripts/Usiigaci/Mask R-CNN')\n",
    "from mrcnn import model as modellib\n",
    "from mrcnn import utils\n",
    "from train import cellConfig\n",
    "\n",
    "import operator\n",
    "from operator import itemgetter\n",
    "from itertools import groupby\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from tifffile import imsave,imread\n",
    "from skimage import exposure\n",
    "from skimage.color import rgb2gray\n",
    "from skimage import io\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import datetime\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "import time\n",
    "from glob import glob\n",
    "import csv\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import colorsys\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "import plotly.plotly as py\n",
    "import plotly.offline as offline\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "IOU_THRESHOLD = 0.6\n",
    "OVERLAP_THRESHOLD = 0.8\n",
    "MIN_DETECTIONS = 1\n",
    "\n",
    "import imagej\n",
    "ij = imagej.init('/home/mestalbet/Fiji.app')\n",
    "import imglyb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read experiment directories <a class=\"anchor\" id=\"chap2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-15T13:52:55.912706Z",
     "start_time": "2018-12-15T13:52:51.951185Z"
    }
   },
   "outputs": [],
   "source": [
    "# files = []\n",
    "# start_dir = \"/home/mestalbet/bucket/BackupData/\"\n",
    "# pattern = \"file.tif\"\n",
    "\n",
    "# for dir, _, _ in os.walk(start_dir):\n",
    "#     files.extend(glob(os.path.join(dir, pattern)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions for processing stacks <a class=\"anchor\" id=\"chap3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-17T18:36:16.597Z"
    },
    "code_folding": [
     0,
     37,
     73,
     78,
     87,
     98,
     125
    ]
   },
   "outputs": [],
   "source": [
    "def alignstack(chosenpath):\n",
    "    temppath = \"/home/mestalbet/PythonScripts/deleteme/mc/\"\n",
    "    # maximum allowed rigid shift in pixels (view the movie to get a sense of motion)\n",
    "    max_shifts = (24, 24)\n",
    "    # create a new patch every x pixels for pw-rigid correction\n",
    "    strides = (128, 128)\n",
    "    # overlap between patches (size of patch strides+overlaps)\n",
    "    overlaps = (48, 48)\n",
    "    # length in frames of each chunk of the movie (to be processed in parallel)\n",
    "    num_frames_split = 50\n",
    "    # maximum deviation allowed for patch with respect to rigid shifts\n",
    "    max_deviation_rigid = 3\n",
    "    # flag for performing rigid or piecewise rigid motion correction\n",
    "    pw_rigid = True  \n",
    "    # flag for correcting motion using bicubic interpolation (otherwise FFT interpolation is used)\n",
    "    shifts_opencv = True\n",
    "    \n",
    "    # replicate values along the boundary (if True, fill in with NaN)\n",
    "    border_nan = 'copy'\n",
    "\n",
    "    if 'dview' in locals():\n",
    "        cm.stop_server(dview=dview)\n",
    "    c, dview, n_processes = cm.cluster.setup_cluster(\n",
    "        backend='local', n_processes=None, single_thread=False)\n",
    "\n",
    "    mc = MotionCorrect(chosenpath, dview=dview, max_shifts=max_shifts,\n",
    "                       strides=strides, overlaps=overlaps,\n",
    "                       max_deviation_rigid=max_deviation_rigid,\n",
    "                       shifts_opencv=shifts_opencv, nonneg_movie=True,\n",
    "                       border_nan=border_nan, use_cuda=True)\n",
    "    mc.motion_correct(save_movie=True)\n",
    "    m_rig = cm.load(mc.mmap_file)\n",
    "    img = np.asarray(m_rig)\n",
    "    if 'dview' in locals():\n",
    "        cm.stop_server(dview=dview)\n",
    "    return(img)\n",
    "\n",
    "#     macro = \"\"\" setBatchMode(true);\n",
    "#                 run(\"Memory & Threads...\", \"maximum=10000 parallel=2\");\n",
    "#                 open(\"{}\");\n",
    "#                 run(\"8-bit\");\n",
    "#                 run(\"Z Project...\", \"projection=[Max Intensity]\");\n",
    "#                 run(\"moco \", \"value=51 downsample_value=0 template=MAX_{} stack={} log=[Generate log file] plot=[No plot]\");\n",
    "#                 saveAs(\"Results\", \"{}\");\n",
    "#                 close();\n",
    "#                 open(\"{}\");\n",
    "#                 run(\"Z Project...\", \"projection=[Max Intensity]\");\n",
    "#                 run(\"moco \", \"value=51 downsample_value=0 template=MAX_{}-1.tif stack={}-1.tif log=[Choose log file] plot=[No plot] choose=\"{}\");\n",
    "#                 saveAs(\"Tiff\", \"{}\");\n",
    "#     \"\"\".format(chosenpath,\n",
    "#                chosenpath.split('/')[-1], \n",
    "#                chosenpath.split('/')[-1],\n",
    "#                '/'.join(chosenpath.split('/')[:-1]) + \"/results.csv\",\n",
    "#                chosenpath,\n",
    "#                chosenpath.split('/')[-1].split('.')[0], \n",
    "#                chosenpath.split('/')[-1].split('.')[0],\n",
    "#               '/'.join(chosenpath.split('/')[:-1]) + \"/results.csv\",\n",
    "#               '/'.join(chosenpath.split('/')[:-1]) + \"/\"+chosenpath.split('/')[-1].split('.')[0]+ \"_aligned.tif\")\n",
    "#     print(macro)\n",
    "#     ij.script().run('Macro.ijm', macro, True).get()\n",
    "#     img = io.imread('/'.join(chosenpath.split('/')[:-1]) + \"/\"+chosenpath.split('/')[-1].split('.')[0]+ \"_aligned.tif\")\n",
    "#     return(img)\n",
    "\n",
    "def improve_raw(impath):\n",
    "    macro = \"\"\"open(\"{}\");\n",
    "               run(\"Normalize Local Contrast\", \"block_radius_x=40 block_radius_y=40 standard_deviations=3 center stretch\");\n",
    "               run(\"Enhance Local Contrast (CLAHE)\", \"blocksize=127 histogram=256 maximum=3 mask=*None*\");\n",
    "               run(\"Gaussian Blur 3D...\", \"x=2 y=2 z=2\");\n",
    "               saveAs(\"TIFF\", \"{}\");\n",
    "               run(\"Close All\");\"\"\".format(impath.replace(\"/\", \"/\"), impath.replace(\"/\", \"/\").replace(\".tif\", \"_clean.tif\"))\n",
    "    ij.script().run('Macro.ijm', macro, True).get()\n",
    "    img = io.imread(impath.replace(\".tif\", \"_clean.tif\"))\n",
    "    return(img)\n",
    "\n",
    "\n",
    "def createmeanimage(alignedpath):\n",
    "    rescale = MinMaxScaler(feature_range=(0, 1))\n",
    "    pt = QuantileTransformer(output_distribution='normal')\n",
    "    temppath = \"D:\\\\PythonScripts\\\\deletme\\\\\"\n",
    "    outpath = os.path.join(temppath, \"kf_temp.tif\")\n",
    "    if os.path.exists(outpath):\n",
    "        os.remove(outpath)\n",
    "    macro = \"\"\"open(\"{}\");\n",
    "    run(\"Kalman Stack Filter\", \"acquisition_noise=0.05 bias=0.80\");\n",
    "    saveAs(\"Tiff\", \"{}\");\n",
    "    run(\"Close All\");\"\"\".format(alignedpath.replace('\\\\', '/'), outpath.replace('\\\\', '/'))\n",
    "    ij.script().run('Macro.ijm', macro, True).get()\n",
    "    img = io.imread(outpath)\n",
    "    img = img.mean(axis=0)\n",
    "    gc.collect()\n",
    "    img = pt.fit_transform(img)\n",
    "    img = rescale.fit_transform(img)\n",
    "    img = exposure.equalize_adapthist(img, clip_limit=0.05)\n",
    "    imrescaled = cv2.resize(img, dsize=(1024, 1024),\n",
    "                            interpolation=cv2.INTER_AREA)\n",
    "    imrescaled = util.invert(imrescaled)\n",
    "    imrescaled = imrescaled*(255**2)\n",
    "    imrescaled = imrescaled.astype('u2')\n",
    "    finalpath = os.path.join(temppath, \"meanimage.tif\")\n",
    "    if os.path.exists(finalpath):\n",
    "        os.remove(finalpath)\n",
    "    io.imsave(finalpath, imrescaled)\n",
    "    imclean = improve_raw(finalpath)\n",
    "    return(imclean)\n",
    "\n",
    "def cleanliorimage(alignedpath):\n",
    "    rescale = MinMaxScaler(feature_range=(0, 1))\n",
    "    pt = QuantileTransformer(output_distribution='normal')\n",
    "    img = io.imread(alignedpath)\n",
    "    img = pt.fit_transform(img)\n",
    "    img = rescale.fit_transform(img)\n",
    "    img = exposure.equalize_adapthist(img, clip_limit=0.05)\n",
    "    imrescaled = cv2.resize(img, dsize=(1024, 1024),\n",
    "                            interpolation=cv2.INTER_AREA)\n",
    "    imrescaled = util.invert(imrescaled)\n",
    "    imrescaled = imrescaled*(255**2)\n",
    "    imrescaled = imrescaled.astype('u2')\n",
    "    io.imsave(os.path.join(alignedpath,\"inference/\"), imrescaled)\n",
    "    return(imrescaled)\n",
    "\n",
    "# get a cell\n",
    "def getcell(mask, cellnum):\n",
    "    cell = mask == cellnum\n",
    "    return(cell.astype(\"float32\"))\n",
    "\n",
    "\n",
    "def random_colors(N):\n",
    "    \"\"\"\n",
    "    Generate brewer colors.\n",
    "    To get visually distinct colors\n",
    "    \"\"\"\n",
    "    colors = sns.color_palette(\"deep\", N)\n",
    "    return(np.asarray(colors))\n",
    "\n",
    "\n",
    "def apply_mask(image, mask, color, alpha=0.3):\n",
    "    \"\"\"Apply the given mask to the image.\n",
    "    \"\"\"\n",
    "    for c in range(3):\n",
    "        image[:, :, c] = np.where(mask == 1,\n",
    "                                  image[:, :, c] *\n",
    "                                  (1 - alpha) + alpha * color[c],\n",
    "                                  image[:, :, c])\n",
    "    return(image)\n",
    "\n",
    "\n",
    "def createoverlay(meanimg, mask):\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    img = cv2.bitwise_not(meanimg)\n",
    "    img = np.dstack([img/(255**2), img/(255**2), img/(255**2)])\n",
    "    numcells = len(np.unique(mask))\n",
    "    colors = random_colors(numcells)\n",
    "    for i in range(1, numcells-1):\n",
    "        cellmask = getcell(mask, i)\n",
    "        x = int(np.round(np.mean(np.where(cellmask > 0)[1])))\n",
    "        y = int(np.round(np.mean(np.where(cellmask > 0)[0])))\n",
    "        img = apply_mask(img, cellmask, colors[i])\n",
    "        img = cv2.putText(img, str(\n",
    "            i-1), (x, y), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255**2, 0), 2, cv2.LINE_AA)\n",
    "    return(img)\n",
    "\n",
    "\n",
    "def gettraces(frames, mask):\n",
    "    numcells = len(np.unique(mask))\n",
    "    traces = []\n",
    "    for cellid in range(1, numcells-1):\n",
    "        cellmask = getcell(mask, cellid)\n",
    "        cellmask[cellmask == 0 ] = np.nan\n",
    "        traces.append([np.nanmean(np.multiply(frame.astype(\"float32\"), cv2.resize(\n",
    "            cellmask, (256, 256)))) for frame in frames])\n",
    "    sig = pd.DataFrame({\"cells\": range(1, numcells-1), \"traces\": traces})\n",
    "    return(sig) #has a bug - it's averaging with many zeros\n",
    "\n",
    "\n",
    "def plotTrace(signals, cellnum):\n",
    "    offline.iplot([go.Scatter(x=np.arange(\n",
    "        len(signals['traces'].iloc[cellnum])), y=signals['traces'].iloc[cellnum])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for calling the trained neural network <a class=\"anchor\" id=\"chap4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-15T13:52:29.596Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def segmentimage(data_dir, model_list):\n",
    "    class Logger(object):\n",
    "        def __init__(self):\n",
    "            self.terminal = sys.stdout\n",
    "#                 self.log = open(data_dir+\"log.log\", \"a\")\n",
    "\n",
    "        def write(self, message):\n",
    "            self.log = open(data_dir+\"log.log\", \"a\")\n",
    "            self.terminal.write(message)\n",
    "            self.log.write(message)\n",
    "            self.log.close()\n",
    "\n",
    "        def flush(self):\n",
    "            pass\n",
    "\n",
    "    class ImageDataset(utils.Dataset):\n",
    "        def load_images(self, dataset_dir):\n",
    "            \"\"\"\n",
    "            Loads dataset images.\n",
    "            :param dataset_dir: string, path to dataset directory.\n",
    "            :return: None\n",
    "            \"\"\"\n",
    "            self.add_class(\"cell\", 1, \"cell\")\n",
    "\n",
    "            image_ids = [fn for fn in os.listdir(dataset_dir)\n",
    "                         if any(fn.endswith(ext) for ext in ['tif', \"png\"])]\n",
    "\n",
    "            for image_id in image_ids:\n",
    "                self.add_image(\n",
    "                    'cell',\n",
    "                    image_id=os.path.splitext(image_id)[0],\n",
    "                    path=os.path.join(dataset_dir, image_id)\n",
    "                )\n",
    "\n",
    "    class CellInferenceConfig(cellConfig):\n",
    "        # Set batch size to 1 to run one image at a time\n",
    "        GPU_COUNT = 1\n",
    "        IMAGES_PER_GPU = 1\n",
    "        # Don't resize imager for inferencing\n",
    "        IMAGE_RESIZE_MODE = \"pad64\"\n",
    "        # Non-max suppression threshold to filter RPN proposals.\n",
    "        # You can increase this during training to generate more propsals.\n",
    "        RPN_NMS_THRESHOLD = 0.7\n",
    "        # define the folder path to data for prediction\n",
    "        global data_dir\n",
    "        all_files = []\n",
    "        sub_directory = []\n",
    "        for root, dirs, files in os.walk(data_dir):\n",
    "            for file in files:\n",
    "                relativePath = os.path.relpath(root, data_dir)\n",
    "                if relativePath == \".\":\n",
    "                    relativePath = \"\"\n",
    "                all_files.append(\n",
    "                    (relativePath.count(os.path.sep), relativePath, file))\n",
    "        all_files.sort(reverse=True)\n",
    "        for (count, folder), files in groupby(all_files, itemgetter(0, 1)):\n",
    "            sub_directory.append(folder)\n",
    "\n",
    "    def detect(model, data_dir, out_dir):\n",
    "        '''\n",
    "        if not os.path.exists(out_dir):\n",
    "            os.makedirs(out_dir)\n",
    "        detection_dir = \"detections_{:%Y%m%dT%H%M%S}\".format(datetime.datetime.now())\n",
    "        detection_dir = os.path.join(out_dir, detection_dir)\n",
    "        os.makedirs(detection_dir)\n",
    "        '''\n",
    "        # Read dataset\n",
    "        dataset = ImageDataset()\n",
    "        dataset.load_images(data_dir)\n",
    "        dataset.prepare()\n",
    "        # Load over images\n",
    "        for image_id in tqdm(dataset.image_ids):\n",
    "            # Load image and run detection\n",
    "            image = dataset.load_image(image_id)\n",
    "            # Detect objects\n",
    "            r = model.detect([image], verbose=0)[0]\n",
    "            # Encode image to RLE. Returns a string of multiple lines\n",
    "            source_id = dataset.image_info[image_id][\"id\"]\n",
    "\n",
    "            #out_path = os.path.join(detection_dir, '%s.png' % str(source_id))\n",
    "            out_path = os.path.join(out_dir, '%s.png' % str(source_id))\n",
    "\n",
    "            mask = np.argmax(r['masks'], 2)\n",
    "            cv2.imwrite(os.path.normpath(out_path), mask)\n",
    "\n",
    "    def compute_iou(mask1, mask2):\n",
    "        \"\"\"\n",
    "        Computes Intersection over Union score for two binary masks.\n",
    "        :param mask1: numpy array\n",
    "        :param mask2: numpy array\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        intersection = np.sum((mask1 + mask2) > 1)\n",
    "        union = np.sum((mask1 + mask2) > 0)\n",
    "\n",
    "        return intersection / float(union)\n",
    "\n",
    "    def compute_overlap(mask1, mask2):\n",
    "        intersection = np.sum((mask1 + mask2) > 1)\n",
    "\n",
    "        overlap1 = intersection / float(np.sum(mask1))\n",
    "        overlap2 = intersection / float(np.sum(mask2))\n",
    "        return overlap1, overlap2\n",
    "\n",
    "    def sort_mask_by_cells(mask, min_size=50):\n",
    "        \"\"\"\n",
    "        Returns size of each cell.\n",
    "        :param mask:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        cell_num = np.unique(mask)\n",
    "        cell_sizes = [(cell_id, len(np.where(mask == cell_id)[0]))\n",
    "                      for cell_id in cell_num if cell_id != 0]\n",
    "\n",
    "        cell_sizes = [x for x in sorted(\n",
    "            cell_sizes, key=lambda x: x[1], reverse=True) if x[1 > min_size]]\n",
    "\n",
    "        return cell_sizes\n",
    "\n",
    "    def merge_multiple_detections(masks):\n",
    "        \"\"\"\n",
    "\n",
    "        :param masks:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        cell_counter = 0\n",
    "        final_mask = np.zeros(masks[0].shape)\n",
    "\n",
    "        masks_stats = [sort_mask_by_cells(mask) for mask in masks]\n",
    "        cells_left = sum([len(stats) for stats in masks_stats])\n",
    "\n",
    "        while cells_left > 0:\n",
    "            # Choose the biggest cell from available\n",
    "            cells = [stats[0][1] if len(\n",
    "                stats) > 0 else 0 for stats in masks_stats]\n",
    "            reference_mask = cells.index(max(cells))\n",
    "\n",
    "            reference_cell = masks_stats[reference_mask].pop(0)[0]\n",
    "\n",
    "            # Prepare binary mask for cell chosen for comparison\n",
    "            cell_location = np.where(masks[reference_mask] == reference_cell)\n",
    "\n",
    "            cell_mask = np.zeros(final_mask.shape)\n",
    "            cell_mask[cell_location] = 1\n",
    "\n",
    "            masks[reference_mask][cell_location] = 0\n",
    "\n",
    "            # Mask for storing temporary results\n",
    "            tmp_mask = np.zeros(final_mask.shape)\n",
    "            tmp_mask += cell_mask\n",
    "\n",
    "            for mask_id, mask in enumerate(masks):\n",
    "                # For each mask left\n",
    "                if mask_id != reference_mask:\n",
    "                    # # Find overlapping cells on other masks\n",
    "                    overlapping_cells = list(np.unique(mask[cell_location]))\n",
    "\n",
    "                    try:\n",
    "                        overlapping_cells.remove(0)\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "\n",
    "                    # # If only one overlapping, check IoU and update tmp mask if high\n",
    "                    if len(overlapping_cells) == 1:\n",
    "                        overlapping_cell_mask = np.zeros(final_mask.shape)\n",
    "                        overlapping_cell_mask[np.where(\n",
    "                            mask == overlapping_cells[0])] = 1\n",
    "\n",
    "                        iou = compute_iou(cell_mask, overlapping_cell_mask)\n",
    "                        if iou >= IOU_THRESHOLD:\n",
    "                            # Add cell to temporary results and remove from stats and mask\n",
    "                            tmp_mask += overlapping_cell_mask\n",
    "                            idx = [i for i, cell in enumerate(\n",
    "                                masks_stats[mask_id]) if cell[0] == overlapping_cells[0]][0]\n",
    "                            masks_stats[mask_id].pop(idx)\n",
    "                            mask[np.where(mask == overlapping_cells[0])] = 0\n",
    "\n",
    "                    # # If more than one overlapping check area overlapping\n",
    "                    elif len(overlapping_cells) > 1:\n",
    "                        overlapping_cell_masks = [\n",
    "                            np.zeros(final_mask.shape) for _ in overlapping_cells]\n",
    "\n",
    "                        for i, cell_id in enumerate(overlapping_cells):\n",
    "                            overlapping_cell_masks[i][np.where(\n",
    "                                mask == cell_id)] = 1\n",
    "\n",
    "                        for cell_id, overlap_mask in zip(overlapping_cells, overlapping_cell_masks):\n",
    "                            overlap_score, _ = compute_overlap(\n",
    "                                overlap_mask, cell_mask)\n",
    "\n",
    "                            if overlap_score >= OVERLAP_THRESHOLD:\n",
    "                                tmp_mask += overlap_mask\n",
    "\n",
    "                                mask[np.where(mask == cell_id)] = 0\n",
    "                                idx = [i for i, cell in enumerate(masks_stats[mask_id])\n",
    "                                       if cell[0] == cell_id][0]\n",
    "                                masks_stats[mask_id].pop(idx)\n",
    "\n",
    "                    # # If none overlapping do nothing\n",
    "\n",
    "            if len(np.unique(tmp_mask)) > 1:\n",
    "                cell_counter += 1\n",
    "                final_mask[np.where(tmp_mask >= MIN_DETECTIONS)] = cell_counter\n",
    "\n",
    "            cells_left = sum([len(stats) for stats in masks_stats])\n",
    "\n",
    "        bin_mask = np.zeros(final_mask.shape)\n",
    "        bin_mask[np.where(final_mask > 0)] = 255\n",
    "\n",
    "        cv2.imwrite(os.path.join(data_dir, 'results/final_bin.png'), bin_mask)\n",
    "        cv2.imwrite(os.path.join(data_dir, 'results/final.png'), final_mask)\n",
    "        return(final_mask)\n",
    "\n",
    "    def process_sequence(masks):\n",
    "        \"\"\"\n",
    "\n",
    "        :param masks:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "    def postprocess(data_dir, out_dir):\n",
    "        \"\"\"\n",
    "\n",
    "        :param data_dir:\n",
    "        :param out_dir:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        #os.makedirs(out_dir, exist_ok=True)\n",
    "        models_dir = [os.path.join(data_dir, filename)\n",
    "                      for filename in os.listdir(data_dir)]\n",
    "        print('Merging multiple models predictions.')\n",
    "        filenames = os.listdir(models_dir[0])\n",
    "\n",
    "        for filename in tqdm(filenames):\n",
    "            masks = [cv2.imread(os.path.join(model_dir, filename), 0)\n",
    "                     for model_dir in models_dir]\n",
    "\n",
    "            result = merge_multiple_detections(masks)\n",
    "            bin_result = np.zeros(result.shape)\n",
    "            bin_result[np.where(result > 0)] = 255\n",
    "\n",
    "            cv2.imwrite(os.path.join(out_dir, filename), result)\n",
    "    all_files = []\n",
    "    sub_directory = []\n",
    "    for root, dirs, files in os.walk(data_dir):\n",
    "        for file in files:\n",
    "            relativePath = os.path.relpath(root, data_dir)\n",
    "            if relativePath == \".\":\n",
    "                relativePath = \"\"\n",
    "            all_files.append(\n",
    "                (relativePath.count(os.path.sep), relativePath, file))\n",
    "    all_files.sort(reverse=True)\n",
    "    for (count, folder), files in groupby(all_files, itemgetter(0, 1)):\n",
    "        sub_directory.append(folder)\n",
    "\n",
    "    config = CellInferenceConfig()\n",
    "    model = modellib.MaskRCNN(\n",
    "        mode=\"inference\", config=config, model_dir=data_dir)\n",
    "    sys.stdout = Logger()\n",
    "\n",
    "    total_start = time.time()\n",
    "    run_time_log = []\n",
    "    for i in sub_directory:\n",
    "        counter = 1\n",
    "        start = time.time()\n",
    "        mask_duplicate_dir = os.path.join(data_dir, i+'_mask')\n",
    "        for m in model_list:\n",
    "            # print(m)\n",
    "            print('>Loading model from: ', m)\n",
    "            model.load_weights(m, by_name=True)\n",
    "            try:\n",
    "                predict_location = os.path.join(data_dir, i)\n",
    "                print('prediction for: ', predict_location)\n",
    "                print('model run '+str(counter)+' of '+str(len(model_list)))\n",
    "                try:\n",
    "                    out_dir = os.path.join(\n",
    "                        mask_duplicate_dir, \"_\"+str(counter))\n",
    "                    os.makedirs(out_dir)\n",
    "                except:\n",
    "                    print('failed to create mask folder')\n",
    "                try:\n",
    "                    detect(model, predict_location, out_dir)\n",
    "                except:\n",
    "                    print('failed to deploy the inference, skipping...')\n",
    "            except:\n",
    "                print('error, skipping...')\n",
    "            counter += 1\n",
    "        try:\n",
    "            avg_prediction_dir = os.path.join(data_dir, i+\"_mask_avg\")\n",
    "            os.makedirs(avg_prediction_dir)\n",
    "        except:\n",
    "            print('failed to create avg mask folder')\n",
    "        postprocess(mask_duplicate_dir, avg_prediction_dir)\n",
    "        end = time.time()\n",
    "        time_diff = end-start\n",
    "        run_time_log.append(time_diff)\n",
    "        hour = time_diff // 3600\n",
    "        time_diff %= 3600\n",
    "        minutes = time_diff // 60\n",
    "        time_diff %= 60\n",
    "        seconds = time_diff\n",
    "        print('prediction run time = %d hr: %d min: %d s' %\n",
    "              (hour, minutes, seconds))\n",
    "    print(run_time_log)\n",
    "    for item in run_time_log:\n",
    "        runtimelogfile = open('exptime.txt', 'w')\n",
    "        runtimelogfile.write(\"%s\\n\" % item)\n",
    "        runtimelogfile.close()\n",
    "    total_end = time.time()\n",
    "    total_time = total_end - total_start\n",
    "    total_day = total_time // (3600*24)\n",
    "    total_time %= (3600*24)\n",
    "    total_hour = total_time // 3600\n",
    "    total_time %= 3600\n",
    "    total_minutes = total_time // 60\n",
    "    total_time %= 60\n",
    "    total_seconds = total_time\n",
    "    print('Total prediction run time = %d day: %d hr: %d min: %d s' %\n",
    "          (total_day, total_hour, total_minutes, total_seconds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Align stack and create denoised mean image <a class=\"anchor\" id=\"chap6\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = \"/home/mestalbet/PythonScripts/Tiff_stacks/file1.tif\"\n",
    "# test =\"/home/mestalbet/bucket/BackupData/180326/3/file.tif\"\n",
    "# alignedstack = alignstack(test)\n",
    "from tensorflow.python.lib.io import file_io\n",
    "with file_io.FileIO('gs://segproj/BackupData/180326/3/file.tif', mode='r') as input_f:\n",
    "    alignedstack = alignstack(input_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.mean(alignedstack,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-15T13:52:29.599Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "model_path_1 = \"/home/mestalbet/PythonScripts/out/latest/mask_rcnn_cell_0100.h5\"\n",
    "model_path_2 = \"/home/mestalbet/PythonScripts/out/latest/mask_rcnn_cell_0198.h5\"\n",
    "model_path_3 = \"/home/mestalbet/PythonScripts/out/latest/mask_rcnn_cell_0199.h5\"\n",
    "model_path_4 = \"/home/mestalbet/PythonScripts/out/latest/mask_rcnn_cell_0200.h5\"\n",
    "model_list = [model_path_1, model_path_2, model_path_3, model_path_4]\n",
    "\n",
    "experimentids = list(set([file.split('/')[5] for file in files]))\n",
    "experimentids = sorted(experimentids)\n",
    "\n",
    "for file in files:\n",
    "    gc.collect()\n",
    "    outpath = \"/home/mestalbet/PythonScripts/Results_LiorImages/\"\n",
    "    tempdir = file.split(\"/\")\n",
    "    chosenpath = \"_\".join([tempdir[-3],tempdir[-2],tempdir[-1]])\n",
    "    alignpath = os.path.join(outpath, chosenpath.replace(\".tif\", \"_aligned.tif\"))\n",
    "    meanpath = os.path.join('/'.join(alignpath.split(\"/\")[:-1]), \"inference/\")\n",
    "    if not os.path.exists(meanpath):\n",
    "        os.makedirs(meanpath)\n",
    "    meanpath = os.path.join(meanpath, alignpath.replace(\n",
    "        \"_aligned.tif\", \"_meanimage.tif\").split(\"/\")[-1])\n",
    "    # Process Chosen Data File\n",
    "    alignedstack = alignstack(file)\n",
    "    imsave(alignpath, alignedstack.astype('int16'), imagej=True)\n",
    "    meanimage = createmeanimage(alignpath)\n",
    "    io.imsave(meanpath, meanimage)\n",
    "# Run Mask RCNN Inference\n",
    "data_dir = \"/\".join(meanpath.split(\"/\")[:-1])+\"/\"\n",
    "gc.collect()\n",
    "segmentimage(data_dir, model_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "outpath = \"/home/mestalbet/PythonScripts/Results_LiorImages/\"\n",
    "alignedpaths = [os.path.join(outpath,f) for f in os.listdir(outpath) if f.endswith(\"tif\")]\n",
    "for alignedpath in alignedpaths:\n",
    "    meanimage = createmeanimage(alignedpath)\n",
    "    gc.collect()\n",
    "    meanpath = os.path.join('/'.join(alignedpath.split(\"/\")[:-1]), \"inference/\")\n",
    "    meanpath = os.path.join(meanpath, alignedpath.replace(\"_aligned.tif\", \"_meanimage.tif\").split(\"\\\\\")[-1])\n",
    "    io.imsave(meanpath, meanimage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from joblib import Parallel, delayed\n",
    "# outpath = \"/home/mestalbet/PythonScripts/Results_LiorImages/\"\n",
    "# alignedpaths = [os.path.join(outpath,f) for f in os.listdir(outpath) if f.endswith(\"tif\")]\n",
    "# Parallel(n_jobs=2)(delayed(cleanliorimage)(alignedpath) for alignedpath in alignedpaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "outpath = \"/home/mestalbet/PythonScripts/Results_LiorImages/\"\n",
    "data_dir = os.path.join(outpath,\"inference/\")\n",
    "model_path_1 = \"/home/mestalbet/PythonScripts/out/latest/mask_rcnn_cell_0100.h5\"\n",
    "model_path_2 = \"/home/mestalbet/PythonScripts/out/latest/mask_rcnn_cell_0198.h5\"\n",
    "model_path_3 = \"/home/mestalbet/PythonScripts/out/latest/mask_rcnn_cell_0199.h5\"\n",
    "model_path_4 = \"/home/mestalbet/PythonScripts/out/latest/mask_rcnn_cell_0200.h5\"\n",
    "model_list = [model_path_1, model_path_2, model_path_3, model_path_4]\n",
    "gc.collect()\n",
    "segmentimage(data_dir, model_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outpath = \"/home/mestalbet/PythonScripts/Results_LiorImages/\"\n",
    "# data_dir = os.path.join(outpath,\"inference/\")\n",
    "# meanfiles = [os.path.join(data_dir,f) for f in os.listdir(data_dir) if f.endswith(\"tif\")]\n",
    "# model_path_1 = \"/home/mestalbet/PythonScripts/out/latest/mask_rcnn_cell_0100.h5\"\n",
    "# model_path_2 = \"/home/mestalbet/PythonScripts/out/latest/mask_rcnn_cell_0198.h5\"\n",
    "# model_path_3 = \"/home/mestalbet/PythonScripts/out/latest/mask_rcnn_cell_0199.h5\"\n",
    "# model_path_4 = \"/home/mestalbet/PythonScripts/out/latest/mask_rcnn_cell_0200.h5\"\n",
    "# model_list = [model_path_1, model_path_2, model_path_3, model_path_4]\n",
    "# gc.collect()\n",
    "\n",
    "# class CellInferenceConfig(cellConfig):\n",
    "#         # Set batch size to 1 to run one image at a time\n",
    "#         GPU_COUNT = 1\n",
    "#         IMAGES_PER_GPU = 1\n",
    "#         # Don't resize imager for inferencing\n",
    "#         IMAGE_RESIZE_MODE = \"pad64\"\n",
    "#         # Non-max suppression threshold to filter RPN proposals.\n",
    "#         # You can increase this during training to generate more propsals.\n",
    "#         RPN_NMS_THRESHOLD = 0.7\n",
    "#         # define the folder path to data for prediction\n",
    "#         global data_dir\n",
    "#         all_files = []\n",
    "#         sub_directory = []\n",
    "#         for root, dirs, files in os.walk(data_dir):\n",
    "#             for file in files:\n",
    "#                 relativePath = os.path.relpath(root, data_dir)\n",
    "#                 if relativePath == \".\":\n",
    "#                     relativePath = \"\"\n",
    "#                 all_files.append(\n",
    "#                     (relativePath.count(os.path.sep), relativePath, file))\n",
    "#         all_files.sort(reverse=True)\n",
    "#         for (count, folder), files in groupby(all_files, itemgetter(0, 1)):\n",
    "#             sub_directory.append(folder)\n",
    "\n",
    "# class ImageDataset(utils.Dataset):\n",
    "#     def load_images(self, dataset_dir):\n",
    "#         \"\"\"\n",
    "#         Loads dataset images.\n",
    "#         :param dataset_dir: string, path to dataset directory.\n",
    "#         :return: None\n",
    "#         \"\"\"\n",
    "#         self.add_class(\"cell\", 1, \"cell\")\n",
    "\n",
    "#         image_ids = [fn for fn in os.listdir(dataset_dir)\n",
    "#                      if any(fn.endswith(ext) for ext in ['tif', \"png\"])]\n",
    "\n",
    "#         for image_id in image_ids:\n",
    "#             self.add_image(\n",
    "#                 'cell',\n",
    "#                 image_id=os.path.splitext(image_id)[0],\n",
    "#                 path=os.path.join(dataset_dir, image_id)\n",
    "#             )\n",
    "\n",
    "# def merge_multiple_detections(masks):\n",
    "#     \"\"\"\n",
    "\n",
    "#     :param masks:\n",
    "#     :return:\n",
    "#     \"\"\"\n",
    "#     cell_counter = 0\n",
    "#     final_mask = np.zeros(masks[0].shape)\n",
    "\n",
    "#     masks_stats = [sort_mask_by_cells(mask) for mask in masks]\n",
    "#     cells_left = sum([len(stats) for stats in masks_stats])\n",
    "\n",
    "#     while cells_left > 0:\n",
    "#         # Choose the biggest cell from available\n",
    "#         cells = [stats[0][1] if len(\n",
    "#             stats) > 0 else 0 for stats in masks_stats]\n",
    "#         reference_mask = cells.index(max(cells))\n",
    "\n",
    "#         reference_cell = masks_stats[reference_mask].pop(0)[0]\n",
    "\n",
    "#         # Prepare binary mask for cell chosen for comparison\n",
    "#         cell_location = np.where(masks[reference_mask] == reference_cell)\n",
    "\n",
    "#         cell_mask = np.zeros(final_mask.shape)\n",
    "#         cell_mask[cell_location] = 1\n",
    "\n",
    "#         masks[reference_mask][cell_location] = 0\n",
    "\n",
    "#         # Mask for storing temporary results\n",
    "#         tmp_mask = np.zeros(final_mask.shape)\n",
    "#         tmp_mask += cell_mask\n",
    "\n",
    "#         for mask_id, mask in enumerate(masks):\n",
    "#             # For each mask left\n",
    "#             if mask_id != reference_mask:\n",
    "#                 # # Find overlapping cells on other masks\n",
    "#                 overlapping_cells = list(np.unique(mask[cell_location]))\n",
    "\n",
    "#                 try:\n",
    "#                     overlapping_cells.remove(0)\n",
    "#                 except ValueError:\n",
    "#                     pass\n",
    "\n",
    "#                 # # If only one overlapping, check IoU and update tmp mask if high\n",
    "#                 if len(overlapping_cells) == 1:\n",
    "#                     overlapping_cell_mask = np.zeros(final_mask.shape)\n",
    "#                     overlapping_cell_mask[np.where(\n",
    "#                         mask == overlapping_cells[0])] = 1\n",
    "\n",
    "#                     iou = compute_iou(cell_mask, overlapping_cell_mask)\n",
    "#                     if iou >= IOU_THRESHOLD:\n",
    "#                         # Add cell to temporary results and remove from stats and mask\n",
    "#                         tmp_mask += overlapping_cell_mask\n",
    "#                         idx = [i for i, cell in enumerate(\n",
    "#                             masks_stats[mask_id]) if cell[0] == overlapping_cells[0]][0]\n",
    "#                         masks_stats[mask_id].pop(idx)\n",
    "#                         mask[np.where(mask == overlapping_cells[0])] = 0\n",
    "\n",
    "#                 # # If more than one overlapping check area overlapping\n",
    "#                 elif len(overlapping_cells) > 1:\n",
    "#                     overlapping_cell_masks = [\n",
    "#                         np.zeros(final_mask.shape) for _ in overlapping_cells]\n",
    "\n",
    "#                     for i, cell_id in enumerate(overlapping_cells):\n",
    "#                         overlapping_cell_masks[i][np.where(\n",
    "#                             mask == cell_id)] = 1\n",
    "\n",
    "#                     for cell_id, overlap_mask in zip(overlapping_cells, overlapping_cell_masks):\n",
    "#                         overlap_score, _ = compute_overlap(\n",
    "#                             overlap_mask, cell_mask)\n",
    "\n",
    "#                         if overlap_score >= OVERLAP_THRESHOLD:\n",
    "#                             tmp_mask += overlap_mask\n",
    "\n",
    "#                             mask[np.where(mask == cell_id)] = 0\n",
    "#                             idx = [i for i, cell in enumerate(masks_stats[mask_id])\n",
    "#                                    if cell[0] == cell_id][0]\n",
    "#                             masks_stats[mask_id].pop(idx)\n",
    "\n",
    "#                 # # If none overlapping do nothing\n",
    "\n",
    "#         if len(np.unique(tmp_mask)) > 1:\n",
    "#             cell_counter += 1\n",
    "#             final_mask[np.where(tmp_mask >= MIN_DETECTIONS)] = cell_counter\n",
    "\n",
    "#         cells_left = sum([len(stats) for stats in masks_stats])\n",
    "#     return(final_mask)\n",
    "\n",
    "# model = modellib.MaskRCNN(mode=\"inference\", config=CellInferenceConfig(), model_dir=data_dir)\n",
    "# model.load_weights(model_path_1, by_name=True)            \n",
    "# dataset = ImageDataset()\n",
    "# dataset.load_images(data_dir)\n",
    "# dataset.prepare()\n",
    "# from joblib import Parallel, delayed\n",
    "# test = []\n",
    "# def detect(i,dataset = dataset, model = model):\n",
    "#     image = dataset.load_image(dataset.image_ids[i])\n",
    "#     r = model.detect([image], verbose=0)[0]\n",
    "#     return(np.argmax(r['masks'], 2))\n",
    "# test = Parallel(n_jobs=8)(delayed(detect)(j) for j in range(0,len(dataset.image_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractsignals(m):\n",
    "    meanpath = os.path.join('/'.join(m.split(\"/\")[:-2]), m.split(\"/\")[-1].replace(\".png\",\".tif\"))\n",
    "    stackpath = os.path.join('/'.join(meanpath.split(\"/\")[:3]),meanpath.split(\"/\")[-1].replace(\"_meanimage.tif\",\"_aligned.tif\"))\n",
    "    csvpath = stackpath.replace(\"_aligned.tif\", \"_extracted_traces.csv\")\n",
    "    mask = io.imread(m)\n",
    "    meanimage = io.imread(meanpath)\n",
    "    img = createoverlay(meanimage, mask.astype('uint16'))\n",
    "    img = img*(255*255)\n",
    "    img = img.astype(\"uint16\")\n",
    "    io.imsave(meanpath.replace(\".tif\", \"_overlay.png\"),img)\n",
    "    alignedstack = imread(stackpath)                        \n",
    "    signals = gettraces(alignedstack, mask)\n",
    "    signals.to_csv(csvpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-15T13:52:29.603Z"
    }
   },
   "outputs": [],
   "source": [
    "outpath = \"/home/mestalbet/PythonScripts/Results_LiorImages/\"\n",
    "mask_dir = os.path.join(data_dir,\"_mask_avg\")\n",
    "mean_dir = os.path.join(data_dir,\"inference/\")\n",
    "# meanpaths = [os.path.join(outpath,f) for f in os.listdir(outpath) if f.endswith(\"tif\")]\n",
    "# alignedpaths = [os.path.join(outpath,f) for f in os.listdir(outpath) if f.endswith(\"tif\")]\n",
    "maskfiles = [os.path.join(mask_dir,f) for f in os.listdir(mask_dir) if f.endswith(\"png\")]\n",
    "\n",
    "for m in maskfiles:\n",
    "    meanpath = os.path.join('/'.join(m.split(\"/\")[:-2]), m.split(\"/\")[-1].replace(\".png\",\".tif\"))\n",
    "    stackpath = os.path.join('/'.join(meanpath.split(\"/\")[:3]),meanpath.split(\"/\")[-1].replace(\"_meanimage.tif\",\"_aligned.tif\"))\n",
    "    csvpath = stackpath.replace(\"_aligned.tif\", \"_extracted_traces.csv\")\n",
    "    mask = io.imread(m)\n",
    "    meanimage = io.imread(meanpath)\n",
    "    img = createoverlay(meanimage, mask.astype('uint16'))\n",
    "    img = img*(255*255)\n",
    "    img = img.astype(\"uint16\")\n",
    "    io.imsave(meanpath.replace(\".tif\", \"_overlay.png\"),img)\n",
    "    alignedstack = imread(stackpath)                        \n",
    "    signals = gettraces(alignedstack, mask)\n",
    "    signals.to_csv(csvpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (segproj)",
   "language": "python",
   "name": "segproj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "notify_time": "30",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "328px",
    "width": "719px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
